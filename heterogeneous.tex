%%
%% Copyright 2007, 2008, 2009 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%%

%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%%
%%
%%
%% $Id: elsarticle-template-num.tex 4 2009-10-24 08:22:58Z rishi $
%%
%%
%%%%%\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%%  \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
 \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%%\documentclass[final,5p,times,twocolumn]{elsarticle}%%DOS COLUMNAS

%% if you use PostScript figures in your article
%% use the graphics package for simple commands
%% \usepackage{graphics}
%% or use the graphicx package for more complicated commands
%% \usepackage{graphicx}
%% or use the epsfig package if you prefer to use the old commands
%% \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{url}
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subfigure}
\usepackage{color}
\usepackage{amsmath}
\usepackage{pifont}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
%% \usepackage{lineno}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}


\journal{Applied Soft Computing}
\providecommand{\e}[1]{\ensuremath{\times 10^{#1}}}
\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{Effect of population size in distributed evolutionary algorithms on heterogeneous clusters}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

\author[ugr]{Pablo Garc\'ia-S\'anchez}
\ead{pgarcia@atc.ugr.es}
\author[ugr]{Gustavo Romero}
\ead{gustavo@ugr.es}
\author[ugr]{Jes\'us Gonz\'alez}
\ead{jesusgonzalez@ugr.es}
\author[ugr]{Antonio Miguel Mora}
\ead{amorag@geneura.ugr.es}
\author[ugr]{Maribel Garc\'ia Arenas}
\ead{maribel@ugr.es}
\author[ugr]{Pedro A. Castillo}
\ead{pedro@atc.ugr.es}
\author[laseeb]{Carlos Fernandes}
\ead{cfernandes@laseeb.org}
\author[ugr]{Juan Juli\'an Merelo}
\ead{jmerelo@geneura.ugr.es}


\address[ugr]{Department of Computer Architecture and Computer Technology and CITIC-UGR, University of Granada, Granada, Spain. Tel: +34958241778. Fax: +34958248993}
\address[laseeb]{LaSEEB-ISR-IST, Technical University of Lisbon (IST), Lisbon, Portugal}%


\begin{abstract}
Most scientists have access to a network of heterogeneous nodes, but
designing distributed algorithms for
heterogeneous clusters so that they leverage all
available resources is still an issue. We are mainly concerned with
distributed evolutionary algorithms, which include a whole range of
parameters with an influence in the performance, however, the size of the
population  is one of the most important, since it has a direct relationship with the
number of iterations
necessary to find the solution. 
% Por favor, mucho cuidado con el argumento aqu√≠, porque no est√° nada
% claro por d√≥nde va. Hay que enmarcar claramente el problema y por
% qu√© a la segunda frase del abstract estamos hablando de la poblaci√≥n
% En todo caso, en la introducci√≥n habr√° que dejar claro por qu√© nos
% centramos en la poblaci√≥n y no en otras cosas. ¬øQu√© estamos
% buscando? ¬øQue se sincronicen todos los nodos? Si es as√≠ hay que
% decirlo arriba. - JJ FERGU: No, no se busca una sincronizaci√≥n tal cual, escribo al principio de la intro un argumento y desarrollar√© desde all√≠.
% El argumento sigue sin estar claro. ``Estamos preocupados con AEs
% distribuidos... donde la poblaci√≥n es muy importante''. ¬øQu√© tiene
% esto que ver con los sistemas heterog√©neos? Para empezar, el marco
% deber√≠a estar en la primera frase: estamos hablando de algoritmos
% evolutivos distribuidos. Para seguir, el problema es usar todos los
% recursos. Para usar todos los recursos, debemos ¬øqu√©? ¬øQue los nodos
% funcionen de forma s√≠ncrona? ¬øQue las prestaciones sean similares?
% ¬øQu√© tiene que ver esto con las prestaciones= Y para hacer eso, lo m√°s importante es
% controlar la poblaci√≥n... Por favor, d√©jalo bien claro porque la
% l√≠nea de argumentaci√≥n es lo m√°s importante del paper - JJ
In this paper, we present a study to
adapt this parameter to the computational power of the nodes of a
heterogeneous cluster, in order to reduce execution time. 
% or to be able to synchronize all nodes? - JJ
% reduce total? Reduce single-node? Why is reducing execution time
% important? Was not the most important thing to leverage all
% available resources? The OBJECTIVE of the paper must be clear and
% the steps taken to make it sure too - JJ
Two size
adaptation schemes have been proposed: an offline and an online
parameter setting, and two problems with different characteristics and
computational demands have been tested. % why only two? Are these
                                % enough? Do they cover a whole range
                                % of possible situations? If they
                                % don't you're up for trouble on the
                                % reviewer front: they can ask for 3,
                                % 4 or $n$ - JJ
Results show
that setting the population size according to the computational power
of the nodes in the heterogeneous cluster decreases the time and
evaluations required to obtain the optimum. %Two different things:
                                %time and evaluations. If they
                                %decrease time due to evaluations,
                                %then it's something: the algorithm
                                %varies. Besides, usually number of
                                %evaluations decrease with the
                                %population in single-population
                                %algorithm. - JJ
 Meanwhile, the same set of
different size values could not improve the time %Improve with respect
                                %to what? - JJ
 in a homogeneous
cluster, so the improvement is due to the interaction of the different
resources with the algorithm. In addition, a study of the influence of
the different population sizes on each stage of the algorithm is
presented. This opens a new research line on the adaptation (offline
or online) of parameters to the computational power of the devices.  % Never done before? If it has, it should go to the state of the art - JJ
 
%Quitado: The total number of individuals is divided taking into account the computational power of each node.
\end{abstract}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
%Service Oriented Architecture \sep OSGi \sep Java \sep Context Management \sep e-health
Evolutionary Algorithms \sep Genetic Algorithms \sep Heterogeneous distributed computation \sep Distributed computing \sep Parameter Tuning \sep Parameter Control
%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
\end{keyword}

\end{frontmatter}

\section{Introduction}
\label{sec:intro}

%ARGUMENTO: 
% 1) Existen nuevas tendencias que usan nodos heterog√©neos
% 2) Adaptar par√°metros de un algoritmo (general) a la potencia de esas m√°quinas heterog√©neas puede hacer que mejore el tiempo de ejecuci√≥n
% 3) En concreto, los EAs pueden adaptarse a estas m√°quinas 


% Y seguimos con las introducciones... todo esto no lleva a nada. Y si
% lleva, tiene que estar en el primer p√°rrafo: ¬øcu√°l es el marco?
% ¬øNodos heterog√©neos? ¬øComputaci√≥n distribuida? Pues al principio - JJ FERGU: adapt√°ndome al ARGUMENTO (arriba) (Nota, la intro la escrib√≠ antes de que me revisaras la tesis, as√≠ que por eso no est√° bien hilado :)

New trends in distributed computing such as Cloud Computing \cite{CLOUD}, GRID
\cite{OPENSCIENCEGRID} or Service Oriented Science \cite{GLOBUS} are
leading to heterogeneous computational devices, including for instance, laptops,
tablets or desktop PCs, working in the same
environment. Thus, many laboratories, which do not include classic
clusters but the usual desktop and laptop computers used by
scientists, should be able to leverage
this motley set as a heterogeneous cluster. 

Adapting algorithm parameters to available heterogeneous computational resources
leads to improved performance
\cite{AutomaticallyConfiguringStyles12}. An easy way to take advantage
of the available resources is  balancing the load
\cite{PARALLELIMPLEMENTATION} so that workloads are to distributed across multiple
elements. However, assigning equal tasks  to each node in
heterogeneous clusters may result in suboptimal performance
\cite{LoadBalancingBohn02}. 




Distributed Evolutionary
Algorithms \cite{MULTIKULTI,PARALLELGRIDHETEROGENEOUS} have been tested successfully in this
type of systems \cite{HETEROGENEOUSHARD} and they have become very popular because their implementation is
not complex.
The most extended model exploits a coarse grained parallelism with sporadic
 communications, being fit to be executed in distributed architectures
 such as clusters or GRIDs \cite{PLATO}.



% -> y aqu√≠ explicas lo que son. O pones este p√°rrafo como primer
% p√°rrafo para enmarcar el trabajo, o lo abrevias y lo pones arriba
% donde lo menciones - JJ FERGU: Reestructurada la intro y movido este p√°rrafo

Evolutionary Algorithms are a general method for solving
optimization and search problems inspired on the evolution of species
and its underlying mechanism, natural selection. These algorithms work
on a population of encoded
possible solutions, called {\em individuals}, that compete using their
{\em fitness} (quality or cost of the solution they encode) with the
rest of the
population. Every iteration of the algorithm (or {\em generation}) the
population evolves by means of selection and recombination/mutation to
create a new set of candidates, until a {\em stop criterion}
(e.g. number of generations) is met. Fitness function is a quality
function that gives the grade of adaptation of an individual respect
the others. This function usually describes the problem to solve. 





Parameters of this kind of algorithms could also be
adapted to increase the performance of the whole system. For example,
the population size in EAs
% Aqu√≠ aparece por primera vez el algoritmo evolutivo -> FERGU: movido de sitio
 is the key to
obtain good performance, because it have effect on the quality of the
solution and the time spent during the run
\cite{ShrinkageLaredo09}. This parameter has been studied as a fixed
\cite{SizingHarik99} or adaptive parameter during runtime
\cite{AdaptiveLobo07,SelfRegulatedSizeFernandes06}, but without taking
into account the computational power of each machine in a
heterogeneous cluster. In this paper we have investigated whether
adapting the population size of the islands of a distributed
Evolutionary Algorithm (dEA) \cite{MULTIKULTI} can leverage the
capability of a heterogeneous cluster. 


%Estabas hablando de computaci√≥n distribuida y ahora hablas de algo
%totalmente diferente. Cada p√°rrafo va por su lado. No hay una
%narrativa.
% Escribe tu argumento en tres o cuatro frases y desarrolla ese
% argumento, cada frase en un p√°rrafo. La argumentaci√≥n debe quedar
% clara. - JJ FERGU: reestructurando 
In distributed EAs a set of nodes executes simultaneously the EA,
working with different sub-populations (or islands) at the same
time. Every certain number of generations one or more individuals are
interchanged (migrated) between sub-populations, which are connected
following a specific topology. Figure \ref{fig:islands} shows this
model with a ring topology.   




\begin{figure}[htb]
\centering
\epsfig{file=ring.eps, width = 7cm}
\caption{Island model scheme using a neighborhood ring topology.}
\label{fig:islands}
\end{figure}



Distributed EAs can be executed on homogeneous clusters with the same
parameters in all nodes (homogeneous dEAs), or with different
parameters or nodes' features (heterogeneous dEAs), in fact,
parameters can be random \cite{garcia2014randomized,tanabe2013evaluation}
It  has also been shown \cite{HETEROGENEOUSHARD} that dEAs with the same parameter configuration are even
more efficient in time and evaluations on heterogeneous hardware configurations than on clusters with
homogeneous devices. This can be explained by different reasons, such
as different memory access times, cache sizes, 
or even implementation
languages or compilers in each machine, leading to a different
exploitation/exploration rate of the search space. % Ein? Sez who?
                                % Why? - JJ
An heterogeneous parameter
configuration  has also been shown to be more  efficient time-wise than a fixed
set 
of parameters for different problems
\cite{HETEROGENEOUSPARAMETERS}.  % so what? - JJ 


These facts have motivated us to study a combination of both ideas in
this paper: dEAs on a heterogeneous set of nodes with different
parameter values adapted to each node. In this study, the parameter to
adapt to the computational power of each node has been the
sub-population size of each island. %why?????? - JJ 


In this work, a heterogeneous distributed system has been used to give an answer to the following questions:
\begin{itemize}
 \item Can a distributed EA be adapted to leverage the capability of a
   heterogeneous cluster? 
 \item How the adaptation of the sub-population size to the computational power affects the execution time and number of evaluations?
 \item Is there any difference between using the same sub-population sizes in a homogeneous and a heterogeneous cluster?
 \item How is each stage of the algorithm (selection, recombination, mutation, replacement and migration) affected by the different
   configurations?
\end{itemize}

%Hasta aqu√≠ no has dicho nada de que lo que vas buscando es una
%sincronizaci√≥n de los nodos o, para el caso, si lo que usas es un
%algoritmo s√≠ncrono o as√≠ncrono. Lo que ser√° importante luego para
%explicar los resultados. - JJ

The rest of the work is structured as follows: after a presentation of
the state of
the art in the algorithm parameter adaptation to computational substrate in dEAs, 
 we present the developed algorithms and experimental setting (Section \ref{sec:experiments}). 
Then, the results of the experiments are shown (Section \ref{sec:results}), followed by conclusions and suggestions for future work lines.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  SOA  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{State of the art}
\label{sec:soa}



One of the problems in parameter adaptation in heterogeneous clusters is 
the computational load representation. It depends of the algorithm, size of the problem, 
language, compiler or hardware characteristics, and the results obtained from artificial 
benchmarks (such as  Linpack \cite{LinpackEndo10}) should not be extolled as identificative 
of the system performance \cite{LinpackDongarra03}. For example, in the work of Garamendi 
et al. \cite{PARALLELIMPLEMENTATION},  a small benchmark was executed in all nodes at the beginning
of the algorithm in order to distribute individuals of an Evolutionary Strategy
 (ES), following a master-slave model. The computational load by artificial benchmarks may not accurately 
 represent the correct load of the algorithm, so, as proposed in this paper, information 
 about the algorithm itself should be used for calibration.

In other works, there are not direct relation with the algorithm parameters and 
computational resources of the nodes. For example, Dom\'inguez et al. \cite{HYDROCM} 
divided the available devices in ``faster'' and ``slower'' nodes to create a distributed hybrid 
meta-heuristic that combines two different EAs: Genetic Algorithms (GAs) and Simulated
Annealing (SA). Their system executes the heavy (in computational
terms) algorithms (GAs) in the faster nodes (computational devices), and
simpler meta-heuristics (SA) in the slower ones, obtaining better results
than other configurations.  Gong et al. in \cite{HETEROGENEOUSTOPOLOGY} also ordered 
the nodes by their computational power to test different topology configurations in a distributed EA.
Besides from ordering the nodes taking into account 
only their previously known computational resources, the results of the previous works were not compared in a homogeneous 
cluster to validate if the adaptation takes advantage of the heterogeneity 
of the cluster, as proposed in this paper.

The heterogeneous computational performance of nodes or network speed can affect the performance of an algorithm. In \cite{HETEROGENEOUSHARD},
 Alba et al. compared a distributed Genetic Algorithm (dGA), one of
 the sub-types of EAs, on homogeneous and heterogeneous clusters. 
 Super-linear performance in terms of iterations was obtained in the heterogeneous ones,
 being more efficient than the same algorithm running on homogeneous
 machines. However, the parameter setting was the same in both
 clusters and they not adapted the parameters to the machines used. 



Adapting algorithm parameters to computational nodes derives in heterogeneous parameter sets. These sets can improve the results in homogeneous hardware, for example, setting a random set of parameters in each homogeneous node can also increase the
performance of a distributed Genetic Algorithm, as explained by Gong
and Fukunaga in \cite{HETEROGENEOUSPARAMETERS}. That model
outperformed a tuned canonical dGA with the same parameter values in
all islands. Other approaches \cite{ParallelGATongchim02,PanaceasClune05} optimize sets of heterogeneous parameters using meta-GAs. Single parameter adaptation have been also studied. For example, adapting the migration rate produced better
results than homogeneous periods, as explained by Salto and Alba in
\cite{HETEROGENEOUSMIGRATION}. This indicates that heterogeneous parameters
 may lead to an increase of performance, so it is necessary to valid if the 
 performance is due to the parameter set or by the heterogeneous devices combination.



In this work we focus in the sub-population sizes, as this parameter has been previously studied in different works because it have a huge impact in the results. In \cite{DifferentialWeber09} two different sub-population sizes are used: size alpha (higher values for exploration) and a variable size beta (lower values for exploitation). In \cite{AdaptationSizesSchlierkamp96} a quality measure is used for each subpopulation to adapt the size, also taking into account the migration and crossover mechanisms. However, those works are based in the competition of sub-populations, taking into account only the information provided by the fitness of the population, and not the machines that are executing the parallel EAs.

%OJO! En AdaptationSizesSchlierkamp96 dicen que cuando no ponen tama√±o global fijo, el tama√±o de las subpoblaciones y los resultados son menos claros (usarlo m√°s alante)
%TODO reescribe bien lo siguiente
In our work, a parameter (size of the sub-populations) of a dEA is adapted (offline and online) to the computational power of each machine, using the information obtained from the algorithm itself, and compared in different hardware systems.
 To the best of our knowledge, there are no works that
 modify parameters of the EA (such as the size) depending of the
 node where the island is being executed, and taking into account information provided by the execution of the algorithm. 


 


%%%%%%%%%%%%%%%%%%  Experiments  %%%%%%%%%%%%%%%%%%%
\section{Experimental setup}
\label{sec:experiments}
In this section we propose two different parameter adaptation schemes to test if adapting the sub-population sizes of a dEA to the nodes of a  heterogeneous clusters reduces time. In the field of  Evolutionary Computation (EC) there are two different approaches about the algorithm parameter setting: {\em parameter tuning} and {\em parameter control} \cite{PARAMETERTUNING}. The first one consists in establishing a good set of parameters before the run (offline), and do not change them during the execution. The parameter control refers to setting up a number of parameters of the EA  and changing these values in running time (online). For the first approach, a dEA has been executed in the heterogeneous cluster. The obtained results have been used, to distribute the number of individual among the nodes (that is, offline). The same sizes set is used in the homogeneous cluster to validate if the changes in performance are dued to the parameters or the adaptation to the nodes. Finally, an online parameter setting that extract relative information of the performance of the nodes has been tested to validate our approach.




\subsection{Algorithm used}
The experimentation is centered in a distributed GA, one of the most used EAs \cite{GeneticAlgorithmsEiben03}. Figure \ref{fig:EA} shows the pseudo-code of the used algorithm.
The algorithm is steady-state, i.e. every generation the offspring is mixed with the parents and the worst individuals are removed. The used neighborhood topology for migration between islands (nodes) is a ring (see Figure \ref{fig:islands}). The best individual is sent to the neighbour in the ring, after a fixed number of generations in each island. The algorithm stops when the optimum (the solution to the problem) is found.  

%Tienes que justificarlo absolutamente todo. ¬øSi se usa otro
%algoritmo, cambiar√≠an los resultados? ¬øPor qu√© se ha usado
%precisamente este? - JJ


\begin{figure}[htb]

\begin{algorithmic}
\STATE population $\gets$ initializePopulation()
\WHILE {stop criterion not met}
    \STATE parents $\gets$ selection(population)
    \STATE offspring $\gets$ recombination(parents)
    \STATE offspring $\gets$ mutation(offspring)
    \STATE population $\gets$ population + offspring
    \IF {time to migrate}
      \STATE migrants $\gets$ selectMigrants(population)
      \STATE remoteBuffer.send(migrants)
    \ENDIF
    \IF {localBuffer.size $\neq$ zero}
      \STATE population $\gets$ population + localBuffer.read()
    \ENDIF
    \STATE population $\gets$ removeWorst(population)
\ENDWHILE

\end{algorithmic}
\caption{Pseudo-code of the used dEA: a distributed Genetic Algorithm (dGA).}
\label{fig:EA}
\end{figure}




\subsection{Problems}
%No empieces con "the problems to evaluate". Di que los resultados
%deber√≠an ser m√°s o menos independientes del problema, pero se han
%elegido estos por tal y cual. Tienes que justificar que con estos es
%suficientes, para que no te digan el cl√°sico "Prueba otro algoritmo"
%- JJ %TODO TERMINAR Y ENLAZAR

Although results may be independent of the problem because an adaptive algorithm can not replace a specific problem adaptation \cite{PanaceasClune05}, different types of problems will be used in this work.

The problems to evaluate are the Massively Multimodal Deceptive
Problem (MMDP) \cite{goldberg92massive}  the OneMax problem
\cite{ONEMAX} and the Rosenbrock Function \cite{CEC2005_Benchmark}. 
% Antonio - Rosenbrock's o 'Shifted Rosenbrock'?
% Rosenbrock. - JJ
Each one requires different actions/abilities by the GA
at the level of population sizing, individual selection and
building-blocks mixing. 

These problems have been previously used in other parameter adaptation algorithms, such as \cite{ParallelGATongchim02} (MMDP and OneMax) and \cite{DifferentialWeber09,AdaptationSizesSchlierkamp96} (Rosenbrock). 

%TODO ESTO DE ARRIBA TIENE QUE PONERSE MEJOR

The MMDP
 is designed to be difficult for an EA, due to
its multimodality and deceptiveness. Deceptive problems are functions where low-order building-blocks do not combine to form higher order building-blocks. Instead, low-order building-blocks may mislead the search towards local optima, thus challenging search mechanisms. MMDP it is composed of $k$ subproblems of 6 bits each one ($s_i$). Depending of
the number of ones (unitation) $s_i$ takes the values shown in Table \ref{table:mmdpvalues}.  

\begin{table}

\centering
{\scriptsize
\caption{ Basic deceptive bipolar function ($s_i$) for MMDP.}
\label{table:mmdpvalues}
\begin{tabular}{|c|c|}
\hline
Unitation&Subfunction value\\
\hline
0 & 1.000000 \\
\hline
1 & 0.000000 \\
\hline
2 & 0.360384 \\
\hline
3 & 0.640576\\
\hline
4 & 0.360384\\
\hline
5 & 0.000000\\
\hline
6 & 1.000000\\
\hline

\end{tabular}
}


\end{table}
%%%%%%%%%%%%%%%%%%

The Shifted Rosenbrock function is defined as:


\begin{equation}\label{eq:rosenbrock}
F_{Rosenbrock}(x)= \sum_{i=1}^{D-1} (100 \cdot (z_{i}^{2}-z_{i+1}^{2}) + 
                                     (z_{i}-1)^2)+f_{bias}
\end{equation}

With $z=x-o+1$ and $x=[x_1,x_2,...,x_D]$, and considering $D$ as the number of dimensions, and $o=[o_1,o_2,...,o_D]$ as the shifted global optimum.

This is a multi-modal shifted function, which is non-separable and scalable. It presents a very narrow valley from local optimum to global optimum.

% Antonio - Fergu, øNecesitas poner el Ûptimo aquÌ?

%%%%%%%%%%%%%%%%%

The fitness value is defined as the sum of the $s_i$ subproblems with an optimum of $k$ (Equation \ref{eq:mmdp}).
The search space is composed of $2^{6k}$ combinations from which there
are only $2^k$ global solutions with $22^k$ deceptive
attractors. Hence, a search method have to find a global solution
out of $2^{5k}$ additionally to deceptiveness. In this work $k=25$. 

\begin{equation}\label{eq:mmdp}
f_{MMDP}(\vec s)= \sum_{i=1}^{k} fitness_{s_i}
\end{equation}

OneMax is a simple linear problem that consists in maximising the number of ones in a binary string. That is, maximize the expression:
\begin{equation}
f_{OneMax}(\vec{x}) = \sum_{i=1}^{N}{x_{i}}
\end{equation}


\subsection{Hardware and parameter configurations}


 % ¬øPor qu√©? ¬øCubren el objetivo
                                % del art√≠culo? Si no, siempre llegar√°
                                % el revisor que te diga "Prueba, no
                                % s√©, "migraci√≥n de poblaciones
                                % completas" o vaya ust√© a saber... -
                                % JJ FERGU: explicado con la siguiente frase

As we are going to test parameter adaptation to hardware, different configurations should be used to compare and validate if the change in the parameters depends only of the parameters, the hardware heterogeneity, or the combination of both.
Five configurations of hardware and parameter settings have been tested:


\begin{itemize}
\item HoSi/HeHa: Homogeneous Size/Heterogeneous Hardware. The same sub-population size in each island on a heterogeneous cluster.
\item HeSi/HeHa: Heterogeneous Size/Heterogeneous Hardware. Different sub-population sizes in each island on a heterogeneous cluster.
\item HoSi/HoHa: Homogeneous Size/Homogeneous Hardware. The same sub-population size in each island on a homogeneous cluster.
\item HeSi/HoHa: Heterogeneous Size/Homogeneous Hardware. Different sub-population sizes (the obtained for HeSi/HeHa) in each island on a homogeneous cluster.

\item AdSi/HeHa: Adaptive Size/Heterogeneous Hardware. Online adaptation of sub-population sizes in each island on a heterogeneous cluster.
\end{itemize}

Two different computational systems have been used: a {\em heterogeneous cluster} and a {\em homogeneous cluster}. The first one is formed by different computers of our lab with different processors, operating systems and memory size. The latter is a dedicated scientific cluster formed by homogeneous nodes. Table \ref{tabcomputers} shows the features of each system and the name of the nodes.

\begin{table*}
\centering{\scriptsize
\caption{Details of the clusters used: a homogeneous cluster (Ho), and a heterogeneous cluster (He)}
\begin{tabular}{|c|c|c|c|c|} \hline
Name     & Processor  & Memory  & Operating System  & Network  \\ \hline
\multicolumn{5}{|c|}{Homogeneous cluster} \\ \hline
HoN[1-4] &  Intel(R) Xeon(R) CPU   E5320  @ 1.86GHz       & 4GB & CentOS 6.7    &   Gigabit Ethernet    \\ \hline
\hline
\multicolumn{5}{|c|}{Heterogeneous cluster} \\ \hline
HeN1  &                &  &   &        \\ \hline
HeN2  &  Intel(R) Core(TM)2 Quad CPU    Q6600  @ 2.40GHz    & 4GB   & Ubuntu 11.04 (64 bits)  & Gigabit Ethernet      \\ \hline
HeN3  &  AMD Phenom(tm) 9950 Quad-Core Processor @ 1.30Ghz    & 3GB   & Ubuntu 10.10 (32 bits)  & 100MB Ethernet      \\ \hline
HeN4  &                &  &   &        \\ \hline
HeN5  &                &  &   &        \\ \hline
HeN6  &  Intel(R) Core(TM)2 Quad CPU    Q6600  @ 2.40GHz    & 4GB   & Ubuntu 11.10 (64 bits)  & Gigabit Ethernet      \\ \hline
HeN7  &                &  &   &        \\ \hline
HeN8  &                &  &   &        \\ \hline
\end{tabular}
\label{tabcomputers}
}
\end{table*}

Also, to study the scalability, two different number of nodes have been used: 4 (from H1 to H4) and 8 (from H1 to H8).

\subsubsection{Homogeneous Size configuration}



In this configuration, each node has the same number of individuals (so, the total amount is 1024). This value has been chosen empirically, as it is big enough to test different sub-population sizes in 4 and 8 nodes. After executing the algorithm 40 times per problem on the heterogeneous cluster, we have obtained the average number of generations in each node, as it can be seen in Table \ref{table:generations}. Note how the generations attained (and their proportion in every node) to reach the optimum depends on the problem considered (besides the hardware).

\begin{table*}
\centering{
\caption{Average number of generations in each node needed to find the
  optimum on the heterogeneous cluster with heterogeneous size.}
% Y la desviaci√≥n? - JJ
\begin{tabular}{|c|c|c|c|c|} \hline
Node        & HeN1     & HeN2      & HeN3     & HeN4   \\ \hline
\multicolumn{5}{|c|}{MMDP problem} \\ \hline
Generations & 10990.25 & 10732.075 &  7721.15 & 717.95 \\ \hline
Proportion  & 36.43    & 35.58    & 25.59    & 2.38    \\ \hline
\multicolumn{5}{|c|}{OneMax problem} \\ \hline
Generations & 2430.27 & 2353.77 & 1423.77 & 91.5 \\ \hline
Proportion  & 38.58   & 37.36   & 22.6   & 1.45 \\ \hline
\end{tabular}
\label{table:generations}
}
\end{table*}


\subsubsection{Heterogeneous Size configuration}

Our aim consists in validating the following hypothesis: adapting the sub-population size to the computational power of the heterogeneous cluster nodes presents an improvement in execution time. 


In this work, for a possible offline way to calculate the computational performance of each node, we have used the average number of generations obtained in the HoSi/HeHa configuration for both problems to determine the computational power of the heterogeneous machines. This comparison takes into account all the evolutionary process in a fair manner (proportional to the memory, processor and network usage), instead a traditional benchmark that usually relies only on the CPU speed. Although this is not obviously the best way, it is a possible way to establish the computational power for the experiments of this work and to determine if changing the sub-population size according the computational power reduces the computing time of the whole approach. It should be considered that the contribution of this work is not the way we have computed these sizes, but compare the algorithm with parameters adapted to their power.

Thus, we have used the obtained average number of generations in the previous sub-section (Table \ref{table:generations}) to set proportionally the sizes in the HeSi/HeHa and HeSi/HoHa configurations, by dividing the total number of individuals (1024). Note that, even having two nodes with the same processors and memory (HeN1 and HeN2), they could have different computational power: this may be produced by different operating systems, virtual machine versions, or number of processes being executed (inside a node).



\subsubsection{Adaptive Size configuration}



Finally, in order to validate the hypothesis that adapt the sub-population sizes to computational resources of a heterogeneous cluster leads to decrease of time for obtain the solution, we propose a third configuration. In this experiment, the adaptation of the sub-population size to the computational power of the islands (nodes) is performed during runtime (online).  Each time a node ($N$) receives an individual, it compares its current number of generations ($Gen_{N}$) with the ones of the node who sent the individual (node $N-1$ in the ring). Then, the sub-population size is adapted proportionally to the difference in the number of generations, following the next equation:

\begin{equation}
size'_{N}=\dfrac{Gen_{N}}{Gen_{N-1}}size_{N}
\end{equation}

If the new size is larger than the actual size, new individuals are added to the sub-population cloning random existent ones. Otherwise, the sub-population must be reduced and thus, the worst are removed.

With this possible online adaptation scheme, each node only requires to receive information of one of the neighbours and not from the whole system. Thus, each node tends to have a number of individuals proportional to their computational power with respect to the other nodes. Experiments on homogeneous cluster do not alter the sub-population sizes, as the number of current generations are equal in all nodes during runtime.

Table \ref{table:parameters} summarizes all the parameters used in the experiments.

\begin{table}
\centering
\caption{Parameters used in all configurations.}
\begin{tabular}{|c|c|} \hline
Name & Value\\ \hline

Crossover type & Two-points crossover \\ \hline
Crossover rate & 0.5\\ \hline
Mutation probability of each gene & 1/individual size\\ \hline
Selection & 2-tournament \\ \hline
Replacement & Worst are removed\\ \hline
Generations to migrate & 64 \\ \hline
Number of individuals to migrate & 1 \\ \hline
Stop criterion & Optimum found \\ \hline
Individual size for MMDP & 150, 240 and 300  \\ \hline
Individual size for OneMax & 5000, 10000 and 15000 \\ \hline
Individual size for Rosenbrock & 10, 30 and 40 \\ \hline
Runs per configuration & 40 \\ \hline
\hline
Total individuals in HoSi and HeSi & 1024\\ \hline \hline
Sub-population size in each node in HoSi (4x) & 256  \\ \hline
Sub-population size in each node in HoSi (8x) & 128  \\ \hline
Sub-population sizes in HeSi for MMDP & 374, 364, 262 and 24 (from N1 to N4)\\ \hline
Sub-population sizes in HeSi for OneMax & 396,  382, 232 and 14 (from N1 to N4)\\ \hline
\hline
Maximum island size in AdSi & 1024 \\ \hline
Minimum island size in AdSi (4x) & 16 \\ \hline
Initial island size in AdSi (4x) & 256 \\ \hline 
Minimum island size in AdSi (8x) & 8 \\ \hline
Initial island size in AdSi (8x) & 128 \\ \hline 
\end{tabular}
\label{table:parameters}
\end{table}

\subsection{Framework}
In order to deal with the operating system and architecture heterogeneity (different operating systems, processors, compilers, etc.), the OSGiLiath framework \cite{SOASOCO}, based in Java, has been used in this work. This is a service-oriented evolutionary framework that automatically configures the services to be used in a local network. In this case, each node offers a migration buffer to accept foreign individuals. Also, in order to reduce bottlenecks in distributed executions, asynchronous communication has been provided to avoid idle time using reception buffers (that is, the algorithm does not wait until new individuals arrive, but the buffers cannot be used again until the reception is done). This kind of communication offers an excellent performance when working with different nodes and operating systems, as demonstrated in \cite{HETEROGENEOUSHARD,AsynchronousMerelo08}. The transmission mechanism is based on ECF Generic server (over TCP)\footnote{\url{http://www.eclipse.org/ecf/}}.  The source code of the algorithms used in this work is available in \url{http://www.osgiliath.org} under a LGPL V3 License. 

%%%%%%%%%%%%%%%%%%  Results  %%%%%%%%%%%%%%%%%%%

\section{Results}
\label{sec:results}



The three main objectives of parallel programming are to tackle large computational problems, increase the performance of algorithms in a finite time, or reduce computational time to solve the problem (reaching the optimum). In this work, we focus in the last objective.
As claimed by Alba and Luque in \cite{EVALUATIONPARALLEL}, assessing the performance of a parallel EA by the number of fitness function evaluations required to attain a solution may be misleading. In our case, for example, the evaluation time is different in each node of the heterogeneous cluster, so the real algorithm speed (in time)could not be reflected correctly. However, the number of evaluations has been included in this section to better understand the results. The total number of generations carried out by all nodes, and the maximum number of generations required by the faster node in each configuration are also shown. It is difficult to compare the performance of HoHa and HeHa for the same reason: the evaluation time is different in each system (and even in each node). Thus, in this work, our aim is not making the heterogeneous cluster comparable or better in time than the homogeneous one (because they are, obviously, different), but showing that the same parameter configuration can improve performance in time on heterogeneous clusters and could not have an effect on homogeneous ones.

\begin{table*}
\centering
\caption{Time to reach the optimum (average $\pm$ std. dev. of 30 runs)  for each configuration, and comparison with the base configuration (HoSi): \ding{115} significantly higher, \ding{116} significantly lower, \ding{117} = no significant difference}
\resizebox{14cm}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|} \hline

				\multicolumn{7}{|c|}{MMDP}																						\\ \hline
\multicolumn{2}{|c|}{}			&	\multicolumn{3}{|c|}{Heterogeneous Hardware}													&	\multicolumn{2}{|c|}{Homogeneous Hardware}								\\ \hline
\multicolumn{2}{|c|}{}			&	Homogeneous Size			&	Heterogeneous Size				&	Adaptive Size				&	Homogeneous Size			&	Heterogeneous Size				\\ \hline
4	&	150	&	23691,400	$\pm$	20349,375	&	12123,575	$\pm$	10535,090	\ding{116}	&	30285,875	$\pm$	33552,447	\ding{117}	&	11560,800	$\pm$	26072,141	&	5500,600	$\pm$	12350,129	\ding{117}	\\ \hline
	&	240	&	347048,675	$\pm$	192450,028	&	199975,150	$\pm$	146997,836	\ding{116}	&	417145,600	$\pm$	269732,017	\ding{117}	&	198431,289	$\pm$	160166,862	&	358710,025	$\pm$	248159,908	\ding{115}	\\ \hline
	&	300	&	1061136,564	$\pm$	373629,143	&	882104,872	$\pm$	465160,080	\ding{116}	&	1228869,800	$\pm$	413829,829	\ding{115}	&	1069118,750	$\pm$	666688,697	&	1264568,050	$\pm$	647531,978	\ding{117}	\\ \hline
8	&	150	&	11348,200	$\pm$	6881,718	&	8055,550	$\pm$	5941,888	\ding{116}	&	11888,525	$\pm$	12928,340	\ding{117}	&	4314,100	$\pm$	10816,704	&	6298,025	$\pm$	12192,268	\ding{117}	\\ \hline
	&	240	&	115213,275	$\pm$	96734,707	&	106284,375	$\pm$	66143,938	\ding{117}	&	146685,800	$\pm$	77926,241	\ding{117}	&	109498,425	$\pm$	98623,989	&	143193,550	$\pm$	165182,806	\ding{117}	\\ \hline
	&	300	&	351729,250	$\pm$	167077,357	&	383083,975	$\pm$	249783,901	\ding{117}	&	434478,025	$\pm$	200879,857	\ding{115}	&	521163,050	$\pm$	304559,018	&	595276,077	$\pm$	355235,861	\ding{117}	\\ \hline
				\multicolumn{7}{|c|}{OneMax}																						\\ \hline
\multicolumn{2}{|c|}{}			&	\multicolumn{3}{|c|}{Heterogeneous Hardware}													&	\multicolumn{2}{|c|}{Homogeneous Hardware}								\\ \hline
\multicolumn{2}{|c|}{}			&	Homogeneous Size			&	Heterogeneous Size				&	Adaptive Size				&	Homogeneous Size			&	Heterogeneous Size				\\ \hline
4	&	5000	&	105413,125	$\pm$	2381,213	&	122794,675	$\pm$	3285,982	\ding{115}	&	137947,400	$\pm$	2716,859	\ding{116}	&	141176,100	$\pm$	2493,730	&	109562,650	$\pm$	2470,219	\ding{116}	\\ \hline
	&	10000	&	427977,875	$\pm$	10484,174	&	538730,250	$\pm$	5669,626	\ding{115}	&	329484,825	$\pm$	9937,048	\ding{116}	&	598873,429	$\pm$	18937,129	&	437625,225	$\pm$	7912,703	\ding{116}	\\ \hline
	&	15000	&	1087096,550	$\pm$	18047,257	&	1415919,250	$\pm$	20477,570	\ding{115}	&	666451,500	$\pm$	17877,417	\ding{116}	&	1510952,275	$\pm$	21107,828	&	1068212,300	$\pm$	25097,388	\ding{116}	\\ \hline
8	&	5000	&	48739,750	$\pm$	1171,984	&	54762,400	$\pm$	779,721	\ding{115}	&	42908,250	$\pm$	1953,038	\ding{116}	&	85041,450	$\pm$	1147,126	&	65303,900	$\pm$	1156,825	\ding{116}	\\ \hline
	&	10000	&	173929,125	$\pm$	2726,059	&	198988,750	$\pm$	2777,299	\ding{115}	&	175120,825	$\pm$	9428,557	\ding{117}	&	320557,300	$\pm$	4661,152	&	235143,225	$\pm$	5530,272	\ding{116}	\\ \hline
	&	15000	&	397398,125	$\pm$	7076,647	&	444353,350	$\pm$	5952,183	\ding{115}	&	430550,775	$\pm$	14095,819	\ding{115}	&	738570,900	$\pm$	10834,293	&	539734,575	$\pm$	8473,574	\ding{116}	\\ \hline
				\multicolumn{7}{|c|}{Shifted Rosenbrock Function}																						\\ \hline
\multicolumn{2}{|c|}{}			&	\multicolumn{3}{|c|}{Heterogeneous Hardware}													&	\multicolumn{2}{|c|}{Homogeneous Hardware}								\\ \hline
\multicolumn{2}{|c|}{}			&	Homogeneous Size			&	Heterogeneous Size				&	Adaptive Size				&	Homogeneous Size			&	Heterogeneous Size				\\ \hline
4	&	10	&	53352,500	$\pm$	27765,191	&	63905,825	$\pm$	26795,632	\ding{115}	&	72567,025	$\pm$	39079,540	\ding{115}	&	144064,750	$\pm$	59634,750	&	133317,350	$\pm$	53464,402	\ding{117}	\\ \hline
	&	30	&	233582,125	$\pm$	159454,007	&	187654,125	$\pm$	133253,223	\ding{117}	&	268306,800	$\pm$	212295,411	\ding{115}	&	225579,325	$\pm$	76117,466	&	273459,025	$\pm$	87878,437	\ding{115}	\\ \hline
	&	50	&	431820,850	$\pm$	294081,294	&	344285,475	$\pm$	205736,331	\ding{117}	&	461542,775	$\pm$	300039,633	\ding{117}	&	411643,275	$\pm$	65338,206	&	553531,700	$\pm$	198818,335	\ding{115}	\\ \hline
8	&	10	&	136191,400	$\pm$	79714,749	&	99235,500	$\pm$	64553,242	\ding{116}	&	64515,275	$\pm$	29709,340	\ding{116}	&	117962,75	$\pm$	35741,6477262521	&	135527,725	$\pm$	38170,1543198266	\ding{117}	\\ \hline
	&	30	&	220603,675	$\pm$	99104,552	&	139196,425	$\pm$	137089,954	\ding{116}	&	158356,275	$\pm$	105886,233	\ding{116}	&	216103,4	$\pm$	21323,0487089945	&	224968,575	$\pm$	39149,2181307704	\ding{117}	\\ \hline
	&	50	&	252010,950	$\pm$	106664,020	&	210020,025	$\pm$	128371,826	\ding{116}	&	283033,825	$\pm$	169912,769	\ding{117}	&	312136,7	$\pm$	81755,2969795559	&	372139,1	$\pm$	63905,4770470038	\ding{115}	\\ \hline


\end{tabular}
}
\label{tab:resultsTIMEall}
\end{table*}


\begin{table*}
\centering
\caption{Number of evaluations (average $\pm$ std. dev. of 30 runs)  for each configuration, and comparison with the base configuration (HoSi): \ding{115} significantly higher, \ding{116} significantly lower, \ding{117} = no significant difference}
\resizebox{14cm}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|} \hline

				\multicolumn{7}{|c|}{MMDP}																									\\ \hline
\multicolumn{2}{|c|}{}			&	\multicolumn{3}{|c|}{Heterogeneous Hardware}															&	\multicolumn{2}{|c|}{Homogeneous Hardware}									\\ \hline
\multicolumn{2}{|c|}{}			&	Homogeneous Size			&	Heterogeneous Size					&	Adaptive Size					&	Homogeneous Size			&	Heterogeneous Size					\\ \hline
4	&	150	&	3321990,4	$\pm$	3640752,79697212	&	1318095,1	$\pm$	1945414,54562334	\ding{116}		&	4495540	$\pm$	6076330,84249115	\ding{117}		&	1314204,8	$\pm$	2963639,113367	&	593926,5	$\pm$	1344934,77944904	\ding{115}		\\ \hline
	&	240	&	43465120	$\pm$	24350006,2785012	&	24865324,65	$\pm$	18722527,7440691	\ding{116}		&	51918846	$\pm$	33924352,604692	\ding{117}		&	20141093,0526316	$\pm$	16264249,5530504	&	29297987,6	$\pm$	20289861,2894758	\ding{115}		\\ \hline
	&	300	&	112163404,8	$\pm$	40760641,0826842	&	94824634,45	$\pm$	43726167,176804	\ding{117}		&	126292378,1	$\pm$	42772727,8796504	\ding{117}		&	91090851,2	$\pm$	56752255,8072305	&	86123573,45	$\pm$	44098884,0834652	\ding{117}		\\ \hline
8	&	150	&	2212630,4	$\pm$	2402352,53373275	&	1143879,95	$\pm$	2251876,90055851	\ding{116}		&	2133441	$\pm$	4344042,55582595	\ding{117}		&	769020,8	$\pm$	1968495,88103461	&	1045242,85	$\pm$	2044957,13354478	\ding{117}		\\ \hline
	&	240	&	33820148,8	$\pm$	29651523,0504766	&	33028780,4	$\pm$	21576055,5258751	\ding{117}		&	44303976,55	$\pm$	24510225,6724239	\ding{117}		&	17062513,6	$\pm$	15386147,5597499	&	19772115,75	$\pm$	22806815,8991615	\ding{115}		\\ \hline
	&	300	&	85830209,6	$\pm$	41366325,3016297	&	92198558,85	$\pm$	60854343,0223991	\ding{117}		&	110130432,3	$\pm$	50703413,2221019	\ding{117}		&	71996897,6	$\pm$	42054877,5798284	&	72278155,1282051	$\pm$	43134165,1178236	\ding{117}		\\ \hline
				\multicolumn{7}{|c|}{OneMax}																									\\ \hline
\multicolumn{2}{|c|}{}			&	\multicolumn{3}{|c|}{Heterogeneous Hardware}															&	\multicolumn{2}{|c|}{Homogeneous Hardware}									\\ \hline
\multicolumn{2}{|c|}{}			&	Homogeneous Size			&	Heterogeneous Size					&	Adaptive Size					&	Homogeneous Size			&	Heterogeneous Size					\\ \hline
4	&	5000	&	762841,6	$\pm$	16566,0196862781	&	910877,55	$\pm$	21772,7162339145	\ding{115}		&	916197,45	$\pm$	23011,2179732586	\ding{116}		&	911238,4	$\pm$	16014,8922693848	&	681001,65	$\pm$	14423,2150751879	\ding{116}		\\ \hline
	&	10000	&	1509209,6	$\pm$	38518,094782609	&	1845115,85	$\pm$	22200,4101389118	\ding{115}		&	1057320,85	$\pm$	35026,1677413272	\ding{116}		&	1194003,4	$\pm$	29300,4464816915	&	874954,2875	$\pm$	15848,2628049403	\ding{115}		\\ \hline
	&	15000	&	2271324,8	$\pm$	46631,0975996018	&	2854326,95	$\pm$	43874,9705085444	\ding{115}		&	1388917,95	$\pm$	46410,4170567563	\ding{116}		&	3021651,7625	$\pm$	42266,9843658671	&	2134754,8	$\pm$	49238,4403453081	\ding{116}		\\ \hline
8	&	5000	&	1022355,2	$\pm$	27173,8572889271	&	1191468,65	$\pm$	18848,8808612213	\ding{115}		&	876971,15	$\pm$	53281,4528078041	\ding{116}		&	1060456	$\pm$	14202,8371308025	&	783309,6	$\pm$	13667,556360284	\ding{115}		\\ \hline
	&	10000	&	1990523,2	$\pm$	33071,2836214325	&	2310726,55	$\pm$	35550,6627454181	\ding{115}		&	1943270,05	$\pm$	109736,643422507	\ding{117}		&	2104908,8	$\pm$	31371,894175585	&	1500722,6	$\pm$	32442,9659942269	\ding{115}		\\ \hline
	&	15000	&	2992969,6	$\pm$	56187,8478461267	&	3447571,1	$\pm$	49039,4312473816	\ding{115}		&	3164844,6	$\pm$	112128,458782613	\ding{115}		&	3189692,8	$\pm$	48537,5308232765	&	2275397,35	$\pm$	37727,2643721032	\ding{115}		\\ \hline
				\multicolumn{7}{|c|}{Shifted Rosenbrock Function}																									\\ \hline
\multicolumn{2}{|c|}{}			&	\multicolumn{3}{|c|}{Heterogeneous Hardware}															&	\multicolumn{2}{|c|}{Homogeneous Hardware}									\\ \hline
\multicolumn{2}{|c|}{}			&	Homogeneous Size			&	Heterogeneous Size					&	Adaptive Size					&	Homogeneous Size			&	Heterogeneous Size					\\ \hline
4	&	10	&	22355824	$\pm$	12904481,3443162	&	30266762,2	$\pm$	13813479,0101399	\ding{115}		&	33547324,1	$\pm$	19863897,7423271	\ding{115}		&	37341065,6	$\pm$	16047434,4374517	&	30153466,3	$\pm$	12576709,7966313	\ding{116}		\\ \hline
	&	30	&	69538364,8	$\pm$	48304416,3986893	&	59728382,4	$\pm$	43501832,2509544	\ding{117}		&	88388447,15	$\pm$	71317459,5481832	\ding{117}		&	46161708,8	$\pm$	15934477,4726121	&	47148156,65	$\pm$	15375823,195559	\ding{115}		\\ \hline
	&	50	&	99155420,8	$\pm$	68311226,2481184	&	84094769,2	$\pm$	50765873,5920968	\ding{117}		&	117038182,7	$\pm$	77175066,543527	\ding{117}		&	68457408	$\pm$	11020886,0100685	&	76027870,2	$\pm$	27583556,0142952	\ding{117}		\\ \hline
8	&	10	&	93296334,4	$\pm$	56652940,8745659	&	72985728,55	$\pm$	50098410,9591695	\ding{117}		&	27464762,4	$\pm$	14325583,0245595	\ding{116}		&	29918763,2	$\pm$	9461803,70148336	&	33237782,15	$\pm$	9691812,09154887	\ding{117}		\\ \hline
	&	30	&	124082083,2	$\pm$	56744693,5501569	&	72921017,2	$\pm$	44154090,2052379	\ding{116}		&	76865581,95	$\pm$	51105012,6488037	\ding{116}		&	50067939,2	$\pm$	5035938,89343552	&	49326413,85	$\pm$	8797658,46342169	\ding{117}		\\ \hline
	&	50	&	118758705,6	$\pm$	51402559,6338874	&	105989246,15	$\pm$	66396359,6234311	\ding{117}		&	119654538,5	$\pm$	73281480,7587221	\ding{117}		&	64545646,4	$\pm$	17131983,2212574	&	68750401,2105263	$\pm$	17422479,1660359	\ding{117}		\\ \hline


\end{tabular}
}
\label{tab:resultsEVALSall}
\end{table*}




\begin{figure}[ht]
\centering

\subfigure{
   \includegraphics[scale =0.5] {boxplots/MMDP_150_TIME.eps}
   \label{fig:150time}
 }
\subfigure{
   \includegraphics[scale =0.5] {boxplots/MMDP_240_TIME.eps}
   \label{fig:240time}
 }
 \subfigure{
   \includegraphics[scale =0.5] {boxplots/MMDP_300_TIME.eps}
   \label{fig:240time}
 }
\caption{Time to obtain the optimum in the MMDP problem (milliseconds).}
\label{fig:timeMMDP}
\end{figure}

\begin{figure}[ht]
\centering

\subfigure{
   \includegraphics[scale =0.5] {boxplots/ONEMAX_5000_TIME.eps}
   \label{fig:5000time}
 }
\subfigure{
   \includegraphics[scale =0.5] {boxplots/ONEMAX_10000_TIME.eps}
   \label{fig:10000time}
 }
 \subfigure{
   \includegraphics[scale =0.5] {boxplots/ONEMAX_15000_TIME.eps}
   \label{fig:15000time}
 }
\caption{Time to obtain the optimum in the OneMax problem (milliseconds).}
\label{fig:timeONEMAX}
\end{figure}

\begin{figure}[ht]
\centering

\subfigure{
   \includegraphics[scale =0.5] {boxplots/ROSENBROCK_10_TIME.eps}
   \label{fig:10time}
 }
\subfigure{
   \includegraphics[scale =0.5] {boxplots/ROSENBROCK_30_TIME.eps}
   \label{fig:30time}
 }
 \subfigure{
   \includegraphics[scale =0.5] {boxplots/ROSENBROCK_50_TIME.eps}
   \label{fig:50time}
 }
\caption{Time needed to obtain the optimum in the Rosenbrock function problem (milliseconds).}
\label{fig:timeROSENBROCK}
\end{figure}










\begin{figure}[ht]
\centering

\subfigure{
   \includegraphics[scale =0.5] {boxplots/MMDP_150_EVALS.eps}
   \label{fig:150evals}
 }
\subfigure{
   \includegraphics[scale =0.5] {boxplots/MMDP_240_EVALS.eps}
   \label{fig:240evals}
 }
 \subfigure{
   \includegraphics[scale =0.5] {boxplots/MMDP_300_EVALS.eps}
   \label{fig:240evals}
 }
\caption{Evaluations to obtain the optimum in the MMDP problem.}
\label{fig:evalsMMDP}
\end{figure}

\begin{figure}[ht]
\centering

\subfigure{
   \includegraphics[scale =0.5] {boxplots/ONEMAX_5000_EVALS.eps}
   \label{fig:5000evals}
 }
\subfigure{
   \includegraphics[scale =0.5] {boxplots/ONEMAX_10000_EVALS.eps}
   \label{fig:10000evals}
 }
 \subfigure{
   \includegraphics[scale =0.5] {boxplots/ONEMAX_15000_EVALS.eps}
   \label{fig:15000evals}
 }
\caption{Evaluations to obtain the optimum in the OneMax problem.}
\label{fig:evalsONEMAX}
\end{figure}

\begin{figure}[ht]
\centering

\subfigure{
   \includegraphics[scale =0.5] {boxplots/ROSENBROCK_10_EVALS.eps}
   \label{fig:10evals}
 }
\subfigure{
   \includegraphics[scale =0.5] {boxplots/ROSENBROCK_30_EVALS.eps}
   \label{fig:30evals}
 }
 \subfigure{
   \includegraphics[scale =0.5] {boxplots/ROSENBROCK_50_EVALS.eps}
   \label{fig:50evals}
 }
\caption{Evaluations needed to obtain the optimum in the Rosenbrock function problem.}
\label{fig:evalsROSENBROCK}
\end{figure}




% No tiene mucho sentido hablar de los resultados de los dos problemas
% por separado, porque el objetivo no es resolver ese problema, sino
% probar c√≥mo funciona en algoritmo. Se pueden comentar los dos juntos
% y se deber√≠a hacer - JJ FERGU: okis, poni√©ndolo todo junto.

Table \ref{tab:resultsMMDP} shows the results for the MMDP problem. These results are also shown in the boxplots of Figure \ref{fig:timeMMDP} (time) and Figure \ref{fig:evalsMMDP} (evaluations). Table \ref{tab:significanceMMDP} shows the statistical significance of the results. First, a Kolmogorov-Smirnov test is performed to assess the normality of the distributions. As all distributions are not normal, we use non-parametric tests. To compare between two methods (HoSi and HeSi in the homogeneous cluster) a Wilcoxon test has been applied. For a three methods comparison (HoSi, HeSi and AdSi on heterogeneous cluster) a Kruskal-Wallis test has been used. %If the results fit a normal distribution, then a Student's T-Test is calculated. Otherwise, the non-parametric Wilcoxon test is applied (see \cite{TUTORIAL} for a tutorial for comparing EAs).

 In the HeHa method, adapting offline the sub-population to the computational
 power of each node makes the algorithm finish significantly earlier,
 and also, needing a lower number of evaluations to reach the solution. On the other hand, in the HoHa system,
 setting the same sub-population sizes makes no difference in time and
 evaluations, that is, changing this parameter has no influence in the
 algorithm's performance (p-value=0.52 for time and 0.08 for evaluations).











To see the differences on how the evolution is being performed, the average fitness in each node of HeHa is shown in Figures \ref{fig:hosiheha} and \ref{fig:hesiheha}. As it can be seen, with the HeSi (Figure \ref{fig:hesiheha}), the local optima are overtaken in less time than HoSi (Figure \ref{fig:hosiheha}).  This can be explained because in HeSi, the migration from HeN4 to HeN1 is performed faster, adding more heterogeneity to the whole system. Gaps in the figures correspond to the time spent in the nodes for sending the migrant individual to other nodes (not while they are receiving them). In the HoHa configurations, the evolution of sub-population is performed at the same time, being the average fitness similar in all nodes during all run. % The natural migration period variation from a processor to another is also giving more diversity to the populations that migrating at the same time of the homogeneous



With respect to AdSi/HeHa, results are significantly  equal (p-value 0.139) to HeSi/HeHa (and, therefore, better than HoSi/HeHa), but this time no previous tuning has been required.  Average sub-population sizes in each node are shown in Table \ref{table:sizesMMDP}. The proportions of size are similar to the proportions in Table \ref{table:generations}. Figure \ref{fig:sizesMMDP} plots all the possible sizes in each node during all the runs. This figure shows that the variation of the sub-population sizes lies proportionally to the computational power of each node. The outliers in boxplots are produced during the size changing, as it can be seen in Figure \ref{fig:sizesMMDP1ejec}. As N4 is the slower node with difference it keeps its size always close to the minimum (16 individuals).


\begin{table*}
\centering{
\caption{Average sub-population size in each node on the heterogeneous cluster with adaptive size (MMDP).}
\begin{tabular}{|c|c|c|c|c|} \hline
Node        & HeN1     & HeN2      & HeN3     & HeN4   \\ \hline
Size &  556.31 & 504.30  & 321.15 & 19.81 \\ \hline
Proportion  & 39.69 &  35.98 & 22.91 & 1.41   \\ \hline
\end{tabular}
\label{table:sizesMMDP}
}
\end{table*}

Summarizing, adapting the population sizes to the
  computational power of each machine (offline and online) has reduced
  the time to obtain the optimum. The same heterogeneous fixed sizes
  in the homogeneous cluster does not produces a significant decrease
  of running time, so the improvement is produced by the heterogeneity
  and not for the different island sizes. Also, the AdSi proposal is
  not applicable here because there are no differences of generations
  during runtime.


% este junto con el anterior y un p√°rrafo al final de los dos a
% comentar conjuntamente los resultados obtenidos. - JJ



Results for this problem are shown in Table \ref{tab:onemaxresults} and Figures  \ref{fig:timeOneMax} and \ref{fig:evalsOneMax}. In this case, adapting offline the sub-population sizes significantly decreases  the running time for solving it in the heterogeneous cluster, but this time, the number of evaluations is increased (see statistical significance in Table \ref{tab:significanceONEMAX}). In the homogeneous system, the effect of changing the sub-population sizes is clearer, and this time the number of evaluations (and therefore, the time) are reduced (both significantly). 

The efficiency on OneMax problem depends mainly on the ability to mix
the building-blocks, and less on the genetic diversity and size of the
population (as with MMDP). No genetic diversity is particularly
required. When properly tuned, a simple Genetic Algorithm is able to
solve OneMax in linear time. Sometimes, problems like OneMax are used
as control functions, in order to check if very efficient algorithms
on hard functions fail on easier ones. As it can be seen in Figure
\ref{fig:gensonemaxhomosize}, the average fitness of all sub-populations
are increasing in linear way in the HoSi/HeHa configuration. However,
the slower node evaluates extremely fewer times.  On the other
% qu√© diablos es un lower processor? slower processor? FERGU: cambiado a slower node y luego m√°s adelante tambi√©n
% por favor revisa muy bien todo esto, que tienes muchos errores
% gramaticales - JJ Fergu: el p√°rrafo anterior lo escribi√≥ Carlos, as√≠ que creo que est√° bien, lo siguiente lo hice yo. He cambiado pronombres err√≥neos y eses en plurales
side, in Figure \ref{fig:gensonemaxheterosize}, smaller sub-population
sizes make that slower nodes increase the number of evaluations,
but the average fitness is also maintained in linear way (and in
smaller increase rate) between migrations. Nevertheless, the other
nodes still perform a higher number of evaluations. That is the
reason why the number of evaluations is higher in HeHa, and lower in
HoHa. Computational time is more efficiently spent in faster nodes,
having a higher chance to cross the individuals. In addition, due to
the larger size of  individuals in the OneMax problem (5000 bits
vs. 150 of the MMDP), the transmission time is larger, (white gaps in the
figures). It also implies that HeN4 sends its best individual to
HeN1 in an extremely large amount of time when using HoSi (every 64
generations). 





%\begin{figure}[ht]
%\centering

%\subfigure[Heterogeneous cluster]{
%   \includegraphics[scale =0.35] {9a.eps}
%   \label{fig:subfig1}
% }
%\subfigure[Homogeneous cluster]{
%   \includegraphics[scale =0.35] {9b.eps}
%   \label{fig:subfig2}
% }
%\caption{Time to obtain the optimum in the OneMax problem (milliseconds).}
%\label{fig:timeOneMax}
%\end{figure}







In the AdSi/HeHa configuration significantly better results in terms of execution time (and number of evaluations) are also attained, and even better than those obtained with HeSi. Average sizes (Table \ref{table:sizesONEMAX}) and boxplots (in Figure \ref{fig:sizesONEMAX}) during all the runs also show proportionality to the computational power of each machine. As in MMDP case, some oscillations (outliers in boxplots) may appear during the execution (as it can be seen in Figure \ref{fig:sizesONEMAX1ejec}).



\begin{table*}
\centering{
\caption{Average sub-population size in each node on the heterogeneous cluster with adaptive size (OneMax).}
\begin{tabular}{|c|c|c|c|c|} \hline
Node        & HeN1     & HeN2      & HeN3     & HeN4   \\ \hline
Size &   267.09 & 158.63 &  74.20  & 16.29 \\ \hline
Proportion  &  51.73 &  30.72 &   14.37 &  3.15  \\ \hline
\end{tabular}
\label{table:sizesONEMAX}
}
\end{table*}










\subsection{Running time analysis}

This sub-section analyses the time spent by each node of the clusters
in every stage of the EA for each configuration with fixed sizes (HoSi
and HeSi). % why? Include always motivation - JJ
Tables \ref{tab:mmdptimes} and \ref{tab:onemaxtimes} show the average and standard deviation of the time spent in each stage of the algorithm (He=Heterogeneous cluster, Ho=Homogeneous cluster). Figures \ref{fig:MMDPbars} and \ref{fig:ONEMAXbars} graphically compare these results. As it can be seen, the migration is the most time consuming operation in all configurations, being the migration in HeHa more expensive than in HoHa. This happens because we are using the multi-purpose laboratory network to communicate the nodes, instead of the specific one used in the HoHa system. Note that the standard deviation of the migration is larger in the HeHa cluster because the network is having real conditions of traffic during the experiment. In the MMDP problem (Table \ref{tab:mmdptimes}) changing the sub-population size does not affect the migration time, but it affects the rest of the algorithm's stages. However, with larger data communications (individuals of 5000 elements of the OneMax problem), the sub-population size affects the migration time of all nodes. This might be due to the synchronization of migration buffers: if the slowest machine is sending/receiving, bottlenecks can be propagated (as it can be seen in Figure \ref{fig:gensonemaxhomosize}). 

Results also show how the stages of the algorithms depends on the node
of execution. %so what ? - JJ
 For example, recombination needs more time than mutation
in both problems only in the node HeN4. The reason might be the
creation of new objects (memory allocation), which in Java and in
limited memory (and swapping) requires more time than the iteration of
elements previously created (for example, in the mutation). Adapting
the sub-population size makes the slower node of HeHa behave in similar
way than the other nodes (same time in each stage). Moreover, the size
of the individuals affects to some parts of the EA; for example, in 
OneMax the mutation requires more time than the replacement. However,
it must be taken into account that the duration of each part of the
algorithm is not related to the time to attain the optimum, but rather to
how the diversity and search guidance is maintained in the whole
system.  % so what? - JJ



\begin{table}[htb]
\centering




\caption{Times needed for all stages of the algorithm for the MMDP
  problem (in ms).}
% Editado. Habr√≠a que editar el resto - JJ
\resizebox{14cm}{!}{
\begin{tabular}{|c|c|c|c|c|c|} \hline
\multicolumn{6}{|c|}{Heterogeneous Cluster} \\ \hline
Node    & Selection     & Recombination     & Mutation      & Replacement       & Migration         \\ \hline
HoSi HeN1    & 0.077 $\pm$  0.170 &  0.788  $\pm$ 0.779  & 1.004  $\pm$ 0.187 &  1.648  $\pm$ 20.185 & 82.458  $\pm$ 143.266 \\ \hline
HoSi HeN2    & 0.088 $\pm$  0.190 &  0.907 $\pm$  0.932  & 1.145  $\pm$ 0.425 &  1.579  $\pm$ 17.907 & 76.725  $\pm$ 126.360\\ \hline
HoSi HeN3    & 0.105 $\pm$  0.163 &  1.207 $\pm$  0.927  & 1.374  $\pm$ 0.301 &  2.108  $\pm$ 21.848 & 108.605 $\pm$ 142.633\\ \hline
HoSi HeN4    & 1.165 $\pm$  1.526 &  30.445$\pm$  59.553 & 12.221 $\pm$ 7.412 &  10.978 $\pm$ 57.135 & 84.936  $\pm$ 0.000\\ \hline \hline
HeSi HeN1    & 0.067 $\pm$  0.065  & 0.973  $\pm$ 0.403 &  1.411 $\pm$  0.166 &  0.790  $\pm$ 6.266 &  28.081 $\pm$ 42.169 \\ \hline
HeSi HeN2    & 0.062 $\pm$  0.075  & 0.973  $\pm$ 0.470 &  1.433 $\pm$  0.265 &  0.811  $\pm$ 7.056 &  29.667 $\pm$ 48.702 \\ \hline
HeSi HeN3    & 0.066 $\pm$  0.108  & 1.104  $\pm$ 0.346 &  1.435 $\pm$  0.296 &  0.937  $\pm$ 7.072 &  40.964 $\pm$ 40.027 \\ \hline
HeSi HeN4    & 0.109 $\pm$  0.257  & 1.895  $\pm$ 5.611 &  0.913 $\pm$  0.834 &  2.085  $\pm$ 5.626 &  43.880 $\pm$ 7.535 \\ \hline 


\multicolumn{6}{|c|}{Homogeneous Cluster} \\ \hline                                 
Node    & Selection     & Recombination     & Mutation      & Replacement       & Migration \\ \hline
HoSi HoN1    & 0.163 $\pm$  0.223 &  1.884 $\pm$  2.386  & 1.591  $\pm$ 0.479 &  2.254  $\pm$ 5.513  & 40.256  $\pm$ 8.726\\ \hline
HoSi HoN2    & 0.151 $\pm$  0.212 &  1.952 $\pm$  2.876  & 1.597  $\pm$ 0.574 &  2.178  $\pm$ 4.922  & 37.110  $\pm$ 6.999\\ \hline
HoSi HoN3    & 0.154 $\pm$  0.206 &  1.990 $\pm$  3.010  & 1.591  $\pm$ 0.577 &  2.215  $\pm$ 4.743  & 36.413  $\pm$ 5.266\\ \hline
HoSi HoN4    & 0.146 $\pm$  0.196 &  1.913 $\pm$  2.697  & 1.651  $\pm$ 1.167 &  2.194  $\pm$ 5.124  & 38.429  $\pm$ 6.192\\ \hline \hline
HeSi HoN1    & 0.214 $\pm$  0.288  & 2.800  $\pm$ 3.793 &  2.359 $\pm$  0.691 &  2.516  $\pm$ 4.706 &  36.972 $\pm$ 4.214 \\ \hline
HeSi HoN2    & 0.190 $\pm$  0.252  & 2.672  $\pm$ 3.902 &  2.277 $\pm$  0.649 &  2.261  $\pm$ 4.546 &  41.171 $\pm$ 9.672 \\ \hline
HeSi HoN3    & 0.148 $\pm$  0.208  & 2.030  $\pm$ 3.161 &  1.623 $\pm$  0.500 &  2.164  $\pm$ 4.512 &  35.551 $\pm$  6.132 \\ \hline
HeSi HoN4    & 0.045 $\pm$  0.052  & 0.345  $\pm$ 1.121 &  0.217 $\pm$  0.142 &  1.531  $\pm$ 4.856 &  38.106 $\pm$ 9.251 \\ \hline
\end{tabular}
}
\label{tab:mmdptimes}
\end{table}


\begin{table}[htb]
\centering
\caption{Times of the stages of the algorithm for the OneMax problem (in ms).}
\resizebox{14cm}{!}{
\begin{tabular}{|c|c|c|c|c|c|} \hline
\multicolumn{6}{|c|}{Heterogeneous Cluster} \\ \hline
Node    & Selection     & Recombination     & Mutation      & Replacement       & Migration         \\ \hline
HoSi HeN1  &  0.048 $\pm$  0.043  & 18.713 $\pm$ 13.454 & 31.984 $\pm$ 2.104  & 18.375 $\pm$ 197.676 & 1172.986  $\pm$  1108.388 \\ \hline
HoSi HeN2  &  0.052 $\pm$  0.051  & 22.266 $\pm$  22.716 & 33.553 $\pm$ 4.931 &  17.176 $\pm$ 180.580 & 1085.508  $\pm$  995.382 \\ \hline
HoSi HeN3  &  0.091 $\pm$  1.005  & 42.634 $\pm$ 21.621  & 47.674 $\pm$ 0.546 &  26.094 $\pm$ 252.667 & 1708.402 $\pm$   1207.925 \\ \hline
HoSi HeN4  &  0.851  $\pm$ 0.435  & 1491.568 $\pm$ 1185.723 & 344.872$\pm$ 6.634 &  5.655  $\pm$ 16.175 & 154.019 $\pm$0.000 \\ \hline \hline
HeSi HeN1 &   0.072 $\pm$  0.063 &  32.917 $\pm$ 26.792 & 49.103 $\pm$ 2.655  & 3.023 $\pm$  27.647 & 163.479 $\pm$157.172 \\ \hline
HeSi HeN2 &   0.080 $\pm$  0.092 &  43.001 $\pm$ 51.680 & 52.288 $\pm$ 13.210 & 2.527 $\pm$  21.861 & 131.063 $\pm$124.404 \\ \hline
HeSi HeN3 &   0.057 $\pm$  0.052 &  33.951 $\pm$ 15.063 & 41.375 $\pm$ 1.707  & 3.284 $\pm$  30.170 & 186.467 $\pm$163.906 \\ \hline
HeSi HeN4 &   0.075 $\pm$  0.107 &  42.443 $\pm$ 88.536 & 16.236 $\pm$ 12.028 & 4.194 $\pm$  33.119 & 131.135 $\pm$144.359 \\ \hline 
\multicolumn{6}{|c|}{Homogeneous Cluster} \\ \hline                                 
Node    & Selection     & Recombination     & Mutation      & Replacement       & Migration \\ \hline
HoSi HoN1  &  0.091 $\pm$  0.078  & 29.969 $\pm$ 21.459 & 47.445 $\pm$ 2.194 &  2.073 $\pm$  6.970 &  38.782 $\pm$ 40.369 \\ \hline
HoSi HoN2  &  0.093 $\pm$  0.082  & 30.119 $\pm$ 22.029 & 47.247 $\pm$ 2.146 &  2.108 $\pm$  7.440 &  44.303 $\pm$ 42.759 \\ \hline
HoSi HoN3  &  0.089 $\pm$  0.080  & 30.951 $\pm$ 21.904 & 47.103 $\pm$ 2.031 &  2.138 $\pm$  8.006 &  46.107 $\pm$ 47.351 \\ \hline
HoSi HoN4  &  0.098 $\pm$  0.075  & 29.468 $\pm$ 20.876 & 47.086 $\pm$ 1.856 &  2.043 $\pm$  7.491 &  41.458 $\pm$ 44.970 \\ \hline \hline
HeSi HoN1 &   0.144 $\pm$  0.151 &  56.124 $\pm$ 48.229 & 72.811 $\pm$ 5.177  & 2.424 $\pm$  9.056  & 48.165  $\pm$57.798 \\ \hline
HeSi HoN2 &   0.141 $\pm$  0.152 &  51.226 $\pm$ 41.016 & 70.047 $\pm$ 4.152  & 2.427 $\pm$  10.890 & 57.152  $\pm$74.177 \\ \hline
HeSi HoN3 &   0.086 $\pm$  0.088 &  26.932 $\pm$ 20.460 & 42.963 $\pm$ 3.935  & 2.239 $\pm$  8.658  & 51.014  $\pm$49.648 \\ \hline
HeSi HoN4 &   0.007 $\pm$  0.008 &  1.215  $\pm$ 1.133  & 2.470  $\pm$ 0.098  & 1.553 $\pm$  10.078 & 50.498 $\pm$ 63.983 \\ \hline
\end{tabular}
}
\label{tab:onemaxtimes}
\end{table}

\section{Conclusions and future work}




In this paper we describe a study on the adaptation of the
sub-population sizes of a distributed EA to the computational power of
the different nodes of an heterogeneous cluster. Two adaptation
schemes (offline and online) that use information of the computational
load of the algorithm have been tested. 
 				% No. Tratamos de
                                % probar un objetivo, dilo
                                % aqu√≠!!!!! - JJ
Results show that adapting (online or offline) the sub-population size
to the computational power of each node in the heterogeneous cluster
yields significantly 
better results in time than keeping the same parameter in all
nodes. This advantage is due to the combination of the heterogeneous
parameters with the heterogeneity of the machines. On the contrary,
the same (heterogeneous) parameter setting in all islands of the
homogeneous cluster could not improve the results than considering the
same parameter value in all nodes. % why? - JJ


 % o sea, tener
                                % maquinas heterog√©neas y par√°metros
                                % heterog√©neos es mejor porque usamos
                                % par√°metros heterog√©neos en m√°quinas
                                % heterog√©neas. Di en qu√© puede
                                % influir eso en la mejora de los
                                % resultados y discute por qu√© podr√≠a
                                % ser as√≠ y prop√≥n experimentos para
                                % probar que efectivamente se trata de
                                % eso. - JJ

Furthermore, changing the sub-population size affects to stages
of the algorithm that are independent of this parameter, such as
the migration. The sub-population size adaptation is also affected by
the problem to solve. %so what? - JJ

In this work, as a possible offline parameter setting, we have calculated the computational power of each node proportionally 
to the average number of generations of the homogeneous parameter set. Moreover, a possible way to adapt 
online the sub-population sizes has been performed comparing the current generation with
 the neighbour generation. These results are a promising starting for adapting EAs to the
performance of each execution node, using more adequate benchmarks or in a dynamic way. 
% Falta una discusi√≥n sobre si las mejoras se deben exclusivamente al
% n√∫mero de evaluaciones o hay alg√∫n otro factor ¬ømenos overhead?
% ¬ønodos m√°s r√°pidos? - JJ FERGU: no, de hecho el n√∫mero de evaluaciones no siempre disminuye, lo digo arriba.

In the future it would be interesting to check the scalability of this
approach, using more computational nodes and larger problem
instances. In addition, other parameters such as migration rate or
crossover probability could be adapted to the execution
nodes. Other appropriate benchmarks to analyse the algorithm will be also used to lead to automatic
parameter adaptation in runtime (online), with different nodes entering or
exiting in the topology, or adapting the parameters to the current load of the
system. 

\section*{Acknowledgements}
This work has been supported in part by 



%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% References
%%
%% Following citation commands can be used in the body text:
%% Usage of \cite is as follows:
%%   \cite{key}         ==>>  [#]
%%   \cite[chap. 2]{key} ==>> [#, chap. 2]
%%

%% References with bibTeX database:

%\bibliographystyle{elsarticle-num}
%\bibliography{AMIVITAL-ESA}

%% Authors are advised to submit their bibtex database files. They are
%% requested to list a bibtex style file in the manuscript if they do
%% not want to use elsarticle-num.bst.

%% References without bibTeX database:

% \begin{thebibliography}{00}

%% \bibitem must have the following form:
%%   \bibitem{key}...
%%

% \bibitem{}

% \end{thebibliography}




\bibliographystyle{plain}
\bibliography{heterogeneous}
%Descomentar abajo cuando comentes arriba
%\section*{References}

%\begin{thebibliography}{10}

%\bibitem{AutomaticallyConfiguringStyles12}
%James Styles, HolgerH. Hoos, and Martin M‚àö¬∫ller.
%\newblock Automatically configuring algorithms for scaling performance.
%\newblock In Youssef Hamadi and Marc Schoenauer, editors, {\em Learning and
%  Intelligent Optimization}, Lecture Notes in Computer Science, pages 205--219.
%  Springer Berlin Heidelberg, 2012.

%\end{thebibliography}

\end{document}

%%
%% End of file `elsarticle-template-num.tex'.
