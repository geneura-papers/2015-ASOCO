%%
%% Copyright 2007, 2008, 2009 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%%

%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%%
%%
%%
%% $Id: elsarticle-template-num.tex 4 2009-10-24 08:22:58Z rishi $
%%
%%
%%%%%\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%%  \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
 \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%%\documentclass[final,5p,times,twocolumn]{elsarticle}%%DOS COLUMNAS

%% if you use PostScript figures in your article
%% use the graphics package for simple commands
%% \usepackage{graphics}
%% or use the graphicx package for more complicated commands
%% \usepackage{graphicx}
%% or use the epsfig package if you prefer to use the old commands
%% \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{url}
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subfigure}
\usepackage{color}
\usepackage{amsmath}
\usepackage{pifont}
\usepackage{multirow}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
%% \usepackage{lineno}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}


\journal{Applied Soft Computing}
\providecommand{\e}[1]{\ensuremath{\times 10^{#1}}}
\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{Effect of population size in distributed evolutionary algorithms on heterogeneous clusters}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

\author[ugr]{Pablo Garc\'ia-S\'anchez}
\ead{pgarcia@atc.ugr.es}
\author[ugr]{Gustavo Romero}
\ead{gustavo@ugr.es}
\author[ugr]{Jes\'us Gonz\'alez}
\ead{jesusgonzalez@ugr.es}
\author[ugr]{Antonio Miguel Mora}
\ead{amorag@geneura.ugr.es}
\author[ugr]{Maribel Garc\'ia Arenas}
\ead{maribel@ugr.es}
\author[ugr]{Pedro A. Castillo}
\ead{pedro@atc.ugr.es}
\author[laseeb]{Carlos Fernandes}
\ead{cfernandes@laseeb.org}
\author[ugr]{Juan Juli\'an Merelo}
\ead{jmerelo@geneura.ugr.es}


\address[ugr]{Department of Computer Architecture and Computer Technology and CITIC-UGR, University of Granada, Granada, Spain. Tel: +34958241778. Fax: +34958248993}
\address[laseeb]{LaSEEB-ISR-IST, Technical University of Lisbon (IST), Lisbon, Portugal}%


\begin{abstract}


Distributed Evolutionary Algorithms (dEAs) are usually executed in homogeneous dedicated clusters, 
but most scientists only have access to networks of heterogeneous nodes (for example, desktop PCs in a lab). 
However, adapting this kind of algorithms to these environments so that they can take advantage of their heterogeneity to save execution time is still an issue.
The different computational power of the nodes may affect the performance of the algorithm, and a proper tuning or adaptation of the algorithm to each node could reduce execution time.

As the dEAs include a whole range of parameters with an influence in the performance, we propose an study on the size of the
population, as it is one of the most important, since it has a direct relationship with the
number of iterations necessary to find the solution. Two size adaptation schemes have been tested: an offline and an online
parameter setting, and three problems with different characteristics and
computational demands have been used.

Results show
that setting the population size according to the computational power
of the nodes in the heterogeneous cluster decreases the time required to obtain the optimum.
Meanwhile, the same set of
different size values could not improve the time time to reach the optimum 
 in a homogeneous
cluster with respect to the same size in all islands, so the improvement is due to the interaction of the different
resources with the algorithm. In addition, a study of the influence of
the different population sizes on each stage of the algorithm is
presented. This opens a new research line on the adaptation (offline
or online) of parameters to the computational power of the devices.



% Por favor, mucho cuidado con el argumento aquí, porque no está nada
% claro por dónde va. Hay que enmarcar claramente el problema y por
% qué a la segunda frase del abstract estamos hablando de la población
% En todo caso, en la introducción habrá que dejar claro por qué nos
% centramos en la población y no en otras cosas. ¿Qué estamos
% buscando? ¿Que se sincronicen todos los nodos? Si es así hay que
% decirlo arriba. - JJ FERGU: No, no se busca una sincronización tal cual, escribo al principio de la intro un argumento y desarrollaré desde allí.
% El argumento sigue sin estar claro. ``Estamos preocupados con AEs
% distribuidos... donde la población es muy importante''. ¿Qué tiene
% esto que ver con los sistemas heterogéneos? Para empezar, el marco
% debería estar en la primera frase: estamos hablando de algoritmos
% evolutivos distribuidos. Para seguir, el problema es usar todos los
% recursos. Para usar todos los recursos, debemos ¿qué? ¿Que los nodos
% funcionen de forma síncrona? ¿Que las prestaciones sean similares?
% ¿Qué tiene que ver esto con las prestaciones= Y para hacer eso, lo más importante es
% controlar la población... Por favor, déjalo bien claro porque la
% línea de argumentación es lo más importante del paper - JJ


 
% or to be able to synchronize all nodes? - JJ
% reduce total? Reduce single-node? Why is reducing execution time
% important? Was not the most important thing to leverage all
% available resources? The OBJECTIVE of the paper must be clear and
% the steps taken to make it sure too - JJ
 % why only two? Are these
                                % enough? Do they cover a whole range
                                % of possible situations? If they
                                % don't you're up for trouble on the
                                % reviewer front: they can ask for 3,
                                % 4 or $n$ - JJ FERGU: mirar arriba el nuevo abstract
 %Two different things:
                                %time and evaluations. If they
                                %decrease time due to evaluations,
                                %then it's something: the algorithm
                                %varies. Besides, usually number of
                                %evaluations decrease with the
                                %population in single-population
                                %algorithm. - JJ
 %Improve with respect
                                %to what? - JJ FERGU: ver arriba
  % Never done before? If it has, it should go to the state of the art - JJ FERGU: TODO Revisa SOA
 
\end{abstract}

\begin{keyword}

Evolutionary Algorithms \sep Genetic Algorithms \sep Heterogeneous distributed computation \sep Distributed computing \sep Parameter Tuning \sep Parameter Control
%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
\end{keyword}

\end{frontmatter}

\section{Introduction}
\label{sec:intro}

%ARGUMENTO: 
% 1) Existen nuevas tendencias que usan nodos heterogéneos
% 2) Adaptar parámetros de un algoritmo (general) a la potencia de esas máquinas heterogéneas puede hacer que mejore el tiempo de ejecución
% 3) En concreto, los EAs pueden adaptarse a estas máquinas 


% Y seguimos con las introducciones... todo esto no lleva a nada. Y si
% lleva, tiene que estar en el primer párrafo: ¿cuál es el marco?
% ¿Nodos heterogéneos? ¿Computación distribuida? Pues al principio - JJ FERGU: adaptándome al ARGUMENTO (arriba) (Nota, la intro la escribí antes de que me revisaras la tesis, así que por eso no está bien hilado :)

%FERGU2: RE-estructurada de nuevo

% -> y aquí explicas lo que son. O pones este párrafo como primer
% párrafo para enmarcar el trabajo, o lo abrevias y lo pones arriba
% donde lo menciones - JJ FERGU: Reestructurada la intro y movido este párrafo al principio

Evolutionary Algorithms are a general method for solving
optimization and search problems inspired on the evolution of species
and its underlying mechanism, natural selection. These algorithms work
on a population of encoded
possible solutions, called {\em individuals}, that compete using their
{\em fitness} (quality or cost of the solution they encode) with the
rest of the
population. Every iteration of the algorithm (or {\em generation}) the
population evolves by means of selection and recombination/mutation to
create a new set of candidates, until a {\em stop criterion}
(e.g. number of generations) is met. Fitness function is a quality
function that gives the grade of adaptation of an individual respect
the others. This function usually describes the problem to solve. 

Parameters of this kind of algorithms could also be
adapted to increase the performance of the whole system. For example,
the population size in EAs
% Aquí aparece por primera vez el algoritmo evolutivo -> FERGU: movido de sitio al principio
 is the key to
obtain good performance, because it have effect on the quality of the
solution and the time spent during the run
\cite{ShrinkageLaredo09}. This parameter has been studied as a fixed
\cite{SizingHarik99} or adaptive parameter during runtime
\cite{AdaptiveLobo07,SelfRegulatedSizeFernandes06}, but without taking
into account the computational power of each machine in a
heterogeneous cluster. In this paper we have investigated whether
adapting the population size of the islands of a distributed
Evolutionary Algorithm (dEA) \cite{MULTIKULTI} can leverage the
capability of a heterogeneous cluster. 

%Estabas hablando de computación distribuida y ahora hablas de algo
%totalmente diferente. Cada párrafo va por su lado. No hay una
%narrativa.
% Escribe tu argumento en tres o cuatro frases y desarrolla ese
% argumento, cada frase en un párrafo. La argumentación debe quedar
% clara. - JJ FERGU: reestructurando la intro
In distributed EAs a set of nodes executes simultaneously the EA,
working with different sub-populations (or islands) at the same
time. Every certain number of generations one or more individuals are
interchanged (migrated) between sub-populations, which are connected
following a specific topology. Figure \ref{fig:islands} shows this
model with a ring topology.   


\begin{figure}[htb]
\centering
\epsfig{file=ring.eps, width = 7cm}
\caption{Island model scheme using a neighborhood ring topology.}
\label{fig:islands}
\end{figure}

This kind of algorithms have been usually executed in homogeneous clusters \cite{MULTIKULTI}, being executed with the same
parameters in all nodes (homogeneous dEAs), or with different
parameters (heterogeneous dEAs) \cite{garcia2014randomized,tanabe2013evaluation,HETEROGENEOUSPARAMETERS}. These configurations can be more  efficient time-wise than a fixed
set of parameters for different problems.

However, new trends in distributed computing such as Cloud Computing \cite{CLOUD}, GRID
\cite{OPENSCIENCEGRID} or Service Oriented Science \cite{GLOBUS} are
leading to heterogeneous computational devices, including for instance, laptops,
tablets or desktop PCs, working in the same
environment. Thus, many laboratories, which do not include classic
clusters but the usual desktop and laptop computers used by
scientists, should be able to leverage
this motley set as a heterogeneous cluster. 

Adapting algorithm parameters to available heterogeneous computational resources
leads to improved performance
\cite{AutomaticallyConfiguringStyles12}. An easy way to take advantage
of the available resources is  balancing the load
\cite{PARALLELIMPLEMENTATION} so that workloads are to distributed across multiple
elements. However, assigning equal tasks  to each node in
heterogeneous clusters may result in suboptimal performance
\cite{LoadBalancingBohn02}. 

It  has also been shown \cite{HETEROGENEOUSHARD} that dEAs with the same parameter configuration are even
more efficient in time and evaluations on heterogeneous hardware configurations than on clusters with
homogeneous devices. This can be explained by different reasons, such
as different memory access times, cache sizes, 
or even implementation
languages or compilers in each machine, leading to a different
exploitation/exploration rate of the search space. % Ein? Sez who?
                                % Why? - JJ

These facts have motivated us to study a combination of both ideas in
this paper: dEAs on a heterogeneous set of nodes with different
parameter values adapted to each node. In this study, the parameter to
adapt to the computational power of each node has been the
sub-population size of each island. %why?????? - JJ FERGU: TODO
%EXPLICAR MEJOR EL POR QUÉ USAR EL TAMAÑO

In this work, a heterogeneous distributed system has been used to give an answer to the following questions:
\begin{itemize}
 \item Can a distributed EA be adapted to leverage the capability of a
   heterogeneous cluster? 
 \item How the adaptation of the sub-population size to the computational power affects the execution time and number of evaluations?
 \item Is there any difference between using the same sub-population sizes in a homogeneous and a heterogeneous cluster?
 \item How is each stage of the algorithm (selection, recombination, mutation, replacement and migration) affected by the different
   configurations?
\end{itemize}

%Hasta aquí no has dicho nada de que lo que vas buscando es una
%sincronización de los nodos o, para el caso, si lo que usas es un
%algoritmo síncrono o asíncrono. Lo que será importante luego para
%explicar los resultados. - JJ

The rest of the work is structured as follows: after a presentation of
the state of
the art in the algorithm parameter adaptation to computational substrate in dEAs, 
 we present the developed algorithms and experimental setting (Section \ref{sec:experiments}). 
Then, the results of the experiments are shown (Section \ref{sec:results}), followed by conclusions and suggestions for future work lines.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  SOA  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{State of the art}
\label{sec:soa}



One of the problems in parameter adaptation in heterogeneous clusters is 
the computational load representation. It depends of the algorithm, size of the problem, 
language, compiler or hardware characteristics, and the results obtained from artificial 
benchmarks (such as  Linpack \cite{LinpackEndo10}) should not be extolled as identificative 
of the system performance \cite{LinpackDongarra03}. For example, in the work of Garamendi 
et al. \cite{PARALLELIMPLEMENTATION},  a small benchmark was executed in all nodes at the beginning
of the algorithm in order to distribute individuals of an Evolutionary Strategy
 (ES), following a master-slave model. The computational load by artificial benchmarks may not accurately 
 represent the correct load of the algorithm, so, as proposed in this paper, information 
 about the algorithm itself should be used for calibration.

In other works, there are not direct relation with the algorithm parameters and 
computational resources of the nodes. For example, Dom\'inguez et al. \cite{HYDROCM} 
divided the available devices in ``faster'' and ``slower'' nodes to create a distributed hybrid 
meta-heuristic that combines two different EAs: Genetic Algorithms (GAs) and Simulated
Annealing (SA). Their system executes the heavy (in computational
terms) algorithms (GAs) in the faster nodes (computational devices), and
simpler meta-heuristics (SA) in the slower ones, obtaining better results
than other configurations.  Gong et al. in \cite{HETEROGENEOUSTOPOLOGY} also ordered 
the nodes by their computational power to test different topology configurations in a distributed EA.
Besides from ordering the nodes taking into account 
only their previously known computational resources, the results of the previous works were not compared in a homogeneous 
cluster to validate if the adaptation takes advantage of the heterogeneity 
of the cluster, as proposed in this paper.

The heterogeneous computational performance of nodes or network speed can affect the performance of an algorithm. In \cite{HETEROGENEOUSHARD},
 Alba et al. compared a distributed Genetic Algorithm (dGA), one of
 the sub-types of EAs, on homogeneous and heterogeneous clusters. 
 Super-linear performance in terms of iterations was obtained in the heterogeneous ones,
 being more efficient than the same algorithm running on homogeneous
 machines. However, the parameter setting was the same in both
 clusters and they not adapted the parameters to the machines used. 



Adapting algorithm parameters to computational nodes derives in heterogeneous parameter sets. These sets can improve the results in homogeneous hardware, for example, setting a random set of parameters in each homogeneous node can also increase the
performance of a distributed Genetic Algorithm, as explained by Gong
and Fukunaga in \cite{HETEROGENEOUSPARAMETERS}. That model
outperformed a tuned canonical dGA with the same parameter values in
all islands. Other approaches \cite{ParallelGATongchim02,PanaceasClune05} optimize sets of heterogeneous parameters using meta-GAs. Single parameter adaptation have been also studied. For example, adapting the migration rate produced better
results than homogeneous periods, as explained by Salto and Alba in
\cite{HETEROGENEOUSMIGRATION}. This indicates that heterogeneous parameters
 may lead to an increase of performance, so it is necessary to valid if the 
 performance is due to the parameter set or by the heterogeneous devices combination.



In this work we focus in the sub-population sizes, as this parameter has been previously studied in different works because it have a huge impact in the results. In the work of Weber et al. \cite{DifferentialWeber09} two different sub-population sizes are used: size alpha (higher values for exploration) and a variable size beta (lower values for exploitation). Schlierkamp-Voosen et al. \cite{AdaptationSizesSchlierkamp96} proposed a quality measure to be used in each subpopulation to adapt the size, also taking into account the migration and crossover mechanisms. However, those works are based in the competition of sub-populations, taking into account only the information provided by the fitness of the population, and not the machines that are executing the parallel EAs.

%OJO! En AdaptationSizesSchlierkamp96 dicen que cuando no ponen tamaño global fijo, el tamaño de las subpoblaciones y los resultados son menos claros (usarlo más alante)
%TODO reescribe bien lo siguiente
In our work, a parameter (size of the sub-populations) of a dEA is adapted (offline and online) to the computational power of each machine, using the information obtained from the algorithm itself, and compared in different hardware systems.
 To the best of our knowledge, there are no works that
 modify parameters of the EA (such as the size) depending of the
 node where the island is being executed, and taking into account information provided by the execution of the algorithm. 


 


%%%%%%%%%%%%%%%%%%  Experiments  %%%%%%%%%%%%%%%%%%%
\section{Experimental setup}
\label{sec:experiments}
In this section we propose two different parameter adaptation schemes to test if adapting the sub-population sizes of a dEA to the nodes of a  heterogeneous clusters reduces time. In the field of  Evolutionary Computation (EC) there are two different approaches about the algorithm parameter setting: {\em parameter tuning} and {\em parameter control} \cite{PARAMETERTUNING}. The first one consists in establishing a good set of parameters before the run (offline), and do not change them during the execution. The parameter control refers to setting up a number of parameters of the EA  and changing these values in running time (online). For the first approach, a dEA has been executed in the heterogeneous cluster. The obtained results have been used, to distribute the number of individual among the nodes (that is, offline). The same sizes set is used in the homogeneous cluster to validate if the changes in performance are dued to the parameters or the adaptation to the nodes. Finally, an online parameter setting that extract relative information of the performance of the nodes has been tested to validate our approach. %TODO esto de validate our approach no mola



\subsection{Algorithm used}
The experimentation is centered in a distributed GA, one of the most used EAs \cite{GeneticAlgorithmsEiben03}. Figure \ref{fig:EA} shows the pseudo-code of the used algorithm.
The algorithm is steady-state, i.e. every generation the offspring is mixed with the parents and the worst individuals are removed. The used neighborhood topology for migration between islands (nodes) is a ring (see Figure \ref{fig:islands}). The best individual is sent to the neighbour in the ring, after a fixed number of generations in each island. The algorithm stops when the optimum (the solution to the problem) is found.  

%Tienes que justificarlo absolutamente todo. ¿Si se usa otro
%algoritmo, cambiarían los resultados? ¿Por qué se ha usado
%precisamente este? - JJ FERGU: TODO


\begin{figure}[htb]

\begin{algorithmic}
\STATE population $\gets$ initializePopulation()
\WHILE {stop criterion not met}
    \STATE parents $\gets$ selection(population)
    \STATE offspring $\gets$ recombination(parents)
    \STATE offspring $\gets$ mutation(offspring)
    \STATE O $\gets$ offspring.size
    \STATE population $\gets$ population + offspring
    
    \IF {time to migrate}
      \STATE migrants $\gets$ selectMigrants(population)
      \STATE remoteBuffer.send(migrants)
    \ENDIF

    \IF {localBuffer.size $\neq$ zero}
      \STATE immigrants $\gets$ localBuffer.read()
      \STATE I $\gets$ immigrants.size()
      \STATE population $\gets$ population + immigrants
    \ENDIF
    
    \IF {localBuffer.size $\neq$ zero}
      \STATE population $\gets$ population + localBuffer.read()
    \ENDIF
    \STATE population $\gets$ population - population.getTheWorst(O + I)
\ENDWHILE

\end{algorithmic}
\caption{Pseudo-code of the used dEA: a distributed Genetic Algorithm (dGA).}
\label{fig:EA}
\end{figure}




\subsection{Problems}
%No empieces con "the problems to evaluate". Di que los resultados
%deberían ser más o menos independientes del problema, pero se han
%elegido estos por tal y cual. Tienes que justificar que con estos es
%suficientes, para que no te digan el clásico "Prueba otro algoritmo"
%- JJ %TODO TERMINAR Y ENLAZAR

Although results may be independent of the problem because an adaptive algorithm can not replace a specific problem adaptation \cite{PanaceasClune05}, different types of problems that cover different characteristics
and computational demands will be used in this work.

The problems to evaluate are the Massively Multimodal Deceptive
Problem (MMDP) \cite{goldberg92massive}  the OneMax problem
\cite{ONEMAX} and the Rosenbrock Function \cite{CEC2005_Benchmark}. 
% Antonio - Rosenbrock's o 'Shifted Rosenbrock'?
% Rosenbrock. - JJ
Each one requires different actions/abilities by the GA
at the level of population sizing, individual selection and
building-blocks mixing. Also, these problems have been previously used in other parameter adaptation algorithms, such as \cite{ParallelGATongchim02} (MMDP and OneMax) and \cite{DifferentialWeber09,AdaptationSizesSchlierkamp96} (Rosenbrock). 

%TODO ESTO DE ARRIBA TIENE QUE PONERSE MEJOR

The MMDP
 is designed to be difficult for an EA, due to
its multimodality and deceptiveness. Deceptive problems are functions where low-order building-blocks do not combine to form higher order building-blocks. Instead, low-order building-blocks may mislead the search towards local optima, thus challenging search mechanisms. MMDP it is composed of $k$ subproblems of 6 bits each one ($s_i$). Depending of
the number of ones (unitation) $s_i$ takes the values shown in Table \ref{table:mmdpvalues}.  

\begin{table}

\centering
{\scriptsize
\caption{ Basic deceptive bipolar function ($s_i$) for MMDP.}
\label{table:mmdpvalues}
\begin{tabular}{|c|c|}
\hline
Unitation&Subfunction value\\
\hline
0 & 1.000000 \\
\hline
1 & 0.000000 \\
\hline
2 & 0.360384 \\
\hline
3 & 0.640576\\
\hline
4 & 0.360384\\
\hline
5 & 0.000000\\
\hline
6 & 1.000000\\
\hline

\end{tabular}
}


\end{table}


The fitness value is defined as the sum of the $s_i$ subproblems with an optimum of $k$ (Equation \ref{eq:mmdp}).
The search space is composed of $2^{6k}$ combinations from which there
are only $2^k$ global solutions with $22^k$ deceptive
attractors. Hence, a search method have to find a global solution
out of $2^{5k}$ additionally to deceptiveness. In this work $k=25$. 

\begin{equation}\label{eq:mmdp}
f_{MMDP}(\vec s)= \sum_{i=1}^{k} fitness_{s_i}
\end{equation}

OneMax is a simple linear problem that consists in maximising the number of ones in a binary string. That is, maximize the expression:
\begin{equation}
f_{OneMax}(\vec{x}) = \sum_{i=1}^{N}{x_{i}}
\end{equation}

%%%%%%%%%%%%%%%%%%

Finally, the Shifted Rosenbrock function is defined as:


\begin{equation}\label{eq:rosenbrock}
F_{Rosenbrock}(x)= \sum_{i=1}^{D-1} (100 \cdot (z_{i}^{2}-z_{i+1}^{2}) + 
                                     (z_{i}-1)^2)+f_{bias}
\end{equation}

With $z=x-o+1$ and $x=[x_1,x_2,...,x_D]$, and considering $D$ as the number of dimensions, and $o=[o_1,o_2,...,o_D]$ as the shifted global optimum.

This is a multi-modal shifted function, which is non-separable and scalable and it presents a very narrow valley from local optimum to global optimum. It is part of the CEC Benchmark \cite{CEC2005_Benchmark}, and the optimum is set to the value 390.

% Antonio - Fergu, ¿Necesitas poner el óptimo aquí?

%%%%%%%%%%%%%%%%%


\subsection{Hardware and parameter configurations}


 % ¿Por qué? ¿Cubren el objetivo
                                % del artículo? Si no, siempre llegará
                                % el revisor que te diga "Prueba, no
                                % sé, "migración de poblaciones
                                % completas" o vaya usté a saber... -
                                % JJ FERGU: explicado con la siguiente frase

As we are going to test parameter adaptation to hardware, different configurations should be used to compare and validate if the change in the parameters depends only of the parameters, the hardware heterogeneity, or the combination of both.
Five configurations of hardware and parameter settings have been tested:


\begin{itemize}
\item HoSi/HeHa: Homogeneous Size/Heterogeneous Hardware. The same sub-population size in each island on a heterogeneous cluster.
\item HeSi/HeHa: Heterogeneous Size/Heterogeneous Hardware. Different sub-population sizes in each island on a heterogeneous cluster.
\item HoSi/HoHa: Homogeneous Size/Homogeneous Hardware. The same sub-population size in each island on a homogeneous cluster.
\item HeSi/HoHa: Heterogeneous Size/Homogeneous Hardware. Different sub-population sizes (the obtained for HeSi/HeHa) in each island on a homogeneous cluster.

\item AdSi/HeHa: Adaptive Size/Heterogeneous Hardware. Online adaptation of sub-population sizes in each island on a heterogeneous cluster.
\end{itemize}

Two different computational systems have been used: a {\em heterogeneous cluster} and a {\em homogeneous cluster}. The first one is formed by different computers of our lab with different processors, operating systems and memory size. The latter is a dedicated scientific cluster formed by homogeneous nodes. Table \ref{tabcomputers} shows the features of each system and the name of the nodes.

\begin{table*}
\centering{\scriptsize
\caption{Details of the clusters used: a homogeneous cluster (Ho), and a heterogeneous cluster (He)}
\begin{tabular}{|c|c|c|c|c|} \hline
Name     & Processor  & Memory  & Operating System  & Network  \\ \hline
\multicolumn{5}{|c|}{Homogeneous cluster} \\ \hline
HoN[1-4] &  Intel(R) Xeon(R) CPU   E5320  @ 1.86GHz       & 4GB & CentOS 6.7    &   Gigabit Ethernet    \\ \hline
\hline
\multicolumn{5}{|c|}{Heterogeneous cluster} \\ \hline
HeN1  &                &  &   &        \\ \hline
HeN2  &  Intel(R) Core(TM)2 Quad CPU    Q6600  @ 2.40GHz    & 4GB   & Ubuntu 11.04 (64 bits)  & Gigabit Ethernet      \\ \hline
HeN3  &  AMD Phenom(tm) 9950 Quad-Core Processor @ 1.30Ghz    & 3GB   & Ubuntu 10.10 (32 bits)  & 100MB Ethernet      \\ \hline
HeN4  &                &  &   &        \\ \hline
HeN5  &                &  &   &        \\ \hline
HeN6  &  Intel(R) Core(TM)2 Quad CPU    Q6600  @ 2.40GHz    & 4GB   & Ubuntu 11.10 (64 bits)  & Gigabit Ethernet      \\ \hline
HeN7  &                &  &   &        \\ \hline
HeN8  &                &  &   &        \\ \hline
\end{tabular}
\label{tabcomputers}
}
\end{table*}

Also, to study the scalability, two different number of nodes have been used in each problem/configuration: 4 nodes (from H1 to H4, defined {\em 4X} configuration for the rest of the paper) and 8 nodes (from H1 to H8, defined {\em 8X} configuration).

\subsubsection{Homogeneous Size configuration}



In this configuration, each node has the same number of individuals (so, the total amount is 1024). This value has been chosen empirically, as it is big enough to test different sub-population sizes in different number of nodes (4 and 8). After executing the algorithm 40 times per problem on the heterogeneous cluster, we have obtained the average number of generations in each node, as it can be seen in Table \ref{table:generations4X} (for 4X) and Table \ref{table:generations8X} (for 8X). Note how the generations attained (and their proportion in every node) to reach the optimum depends on the problem considered (besides the hardware).

\begin{table*}
\centering{
\caption{Average number of generations in each node of the HeHa/HoSi/4X configuration when the optimum is found. These values are used to set proportionally the sub-population sizes in HeSi/4X.}
% Y la desviación? - JJ
\begin{tabular}{|c|c|c|c|c|c|} \hline
Instance &         & HeN1     & HeN2      & HeN3     & HeN4   \\ \hline
\multicolumn{6}{|c|}{MMDP problem} \\ \hline
\multirow{3}{*}{150} & Generations   & 25438  & 17515  &  14030  & 642 \\ \cline{2-6}
                     & Proportion    & 44.14  & 30.39  & 24.34   &  1.11   \\ \cline{2-6}
                     & Sub-pop. size & 452    & 311    & 249     &  12       \\ \hline
\multirow{3}{*}{240} & Generations   & 214425 & 159545 & 122233  & 6899  \\ \cline{2-6}
                     & Proportion    & 42.62  &  31.71 &  24.30  & 1.37       \\ \cline{2-6}
                     & Sub-pop. size & 436    &  324   & 248     & 14      \\ \hline
\multirow{3}{*}{300} & Generations   & 380205 & 276780 & 203157  & 7730     \\ \cline{2-6}
                     & Proportion    & 43.81  & 31.89  & 23.41   & 0.89    \\ \cline{2-6}
                     & Sub-pop. size & 448    &  326   & 240     & 10    \\ \hline                     
\multicolumn{6}{|c|}{OneMax problem} \\ \hline
\multirow{3}{*}{5000} & Generations   & 2486  &  1575  & 1377    & 512      \\ \cline{2-6}
                     & Proportion    &  41.78 &  26.47 &  23.14  & 8.61   \\ \cline{2-6}
                     & Sub-pop. size &  428   &   272  & 236     & 88    \\ \hline    
\multirow{3}{*}{10000} & Generations   & 5135 & 2862   & 2658    & 1125      \\ \cline{2-6}
                     & Proportion    &  43.59 &  24.30 &  22.56  & 9.55     \\ \cline{2-6}
                     & Sub-pop. size &  446   & 248    &  232    & 98   \\ \hline    
\multirow{3}{*}{15000} & Generations & 7922   & 4068   & 4092    & 1653      \\ \cline{2-6}
                     & Proportion    &  44.67 &  22.94 &  23.07  & 9.32     \\ \cline{2-6}
                     & Sub-pop. size & 457    & 235    & 236 & 96      \\ \hline    
\multicolumn{6}{|c|}{Rosenbrock's problem} \\ \hline
\multirow{3}{*}{10} & Generations   & 74736   &47019   & 52293   & 601     \\ \cline{2-6}
                     & Proportion    & 42.79  &  26.92 &   29.94 &   0.34    \\ \cline{2-6}
                     & Sub-pop. size &  441  & 270     & 309     & 4      \\ \hline    
\multirow{3}{*}{30} & Generations   	&  241543 &   142761 & 156516 & 3175     \\ \cline{2-6}
                     & Proportion    & 44.40 &   26.24   & 28.77  & 0.58   \\ \cline{2-6}
                     & Sub-pop. size & 451 &  276 & 291 & 6       \\ \hline    
\multirow{3}{*}{50} & Generations   &  344159 &  211725 & 212865 & 5898    \\ \cline{2-6}
                     & Proportion    & 44.43  &  27.33  & 27.48  & 0.76     \\ \cline{2-6}
                     & Sub-pop. size & 456 & 280 & 280 & 8      \\ \hline  
\end{tabular}
\label{table:generations4X}
}
\end{table*}

\begin{table*}
\centering{
\caption{Average number of generations in each node of the HeHa/HoSi/8X configuration when the optimum is found. These values are used to set proportionally the sub-population sizes in HeSi/8X.}
\resizebox{14cm}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|} \hline
Instance &                       & HeN1   & HeN2   & HeN3    & HeN4  & HeN5     & HeN6      & HeN7    & HeN8  \\ \hline
\multicolumn{10}{|c|}{MMDP problem} \\ \hline
\multirow{3}{*}{150} & Generations   &   5315 &  3808.52 &    3385.95 & 85.625 & 6644.525   & 4341.85 & 5696.875  &  5278    \\ \cline{2-10}
                     & Proportion    &   15.38&  11.02  & 9.80   & 0.25  &  19.23  & 12.56  & 16.49 &  15.27    \\ \cline{2-10}
                     & Sub-pop. size &   157  &  113   & 100     & 4  & 197 & 128 & 169 & 156             \\ \hline
\multirow{3}{*}{240} & Generations   &   60946.95 &  58732.60  &  54923.65 &   1559.03 & 65228.90  &  53067.33 &   60614.33   & 63086.53            \\ \cline{2-10}
                     & Proportion    &   14.58 & 14.05  & 13.13  & 0.37   & 15.60  & 12.69   & 14.50  & 15.09    \\ \cline{2-10}
                     & Sub-pop. size &   149  &  143 & 134&  4 &  159&  130 & 148 & 154           \\ \hline
\multirow{3}{*}{300} & Generations   &   202994.90 & 149616.83 &  121461.65 &  5109.53& 269693.13 &  126157.25 &  243472.45   &222575.30        \\ \cline{2-10}
                     & Proportion    &   15.14  & 11.16 &  9.06 &   0.38  &  20.11&   9.41  &  18.15  & 16.60    \\ \cline{2-10}
                     & Sub-pop. size &   155    & 114 &93  &4  & 206& 96 & 186 &170       \\ \hline                     
\multicolumn{10}{|c|}{OneMax problem} \\ \hline
\multirow{3}{*}{5000} & Generations  &  2138.28  & 1547.13  & 1226.78  & 331.28  &  3203.85 &  1075.45  & 2978.15 &  2561.90     \\ \cline{2-10}
                     & Proportion    &  14.20  &   10.27  &   8.14   &   2.20   &   21.27   &  7.14  &    19.77  &   17.01          \\ \cline{2-10}
                     & Sub-pop. size &  145  & 105  & 83 &   24  &  218 &  73  &  202  & 174         \\ \hline    
\multirow{3}{*}{10000} & Generations &  4173.28 &  2837.33  & 2399.25  & 770.00  &  6460.45  & 2220.53  & 5618.03 &  4977.83         \\ \cline{2-10}
                     & Proportion    &  14.17  &   9.63   &   8.15    &  2.61  &    21.93    & 7.54  &    19.07  &   16.90          \\ \cline{2-10}
                     & Sub-pop. size &   145    &  99  &  84  &  27  &  224  & 77 &   195  & 173          \\ \hline    
\multirow{3}{*}{15000} & Generations &   6420.63  &    4059.58  & 3374.20  & 1315.00  & 9734.38  & 3378.35  & 8823.65  & 7584.18       \\ \cline{2-10}
                     & Proportion    &   14.37  &  9.08   &   7.55   &   2.94   &   21.78   &  7.56   &   19.74   &  16.97    \\ \cline{2-10}
                     & Sub-pop. size &   147   &   93  &  77  &  30  &  223  & 78  &  202 &  174           \\ \hline    
\multicolumn{10}{|c|}{Rosenbrock's problem} \\ \hline
\multirow{3}{*}{10} & Generations    &  191564.30   &  156714.43  &   164188.25   &  1734.53  & 222300.15  &   177448.80  &   196226.95  &   185116.10          \\ \cline{2-10}
                     & Proportion    &  14.79   &  12.10   &  12.68   &  0.13   &   17.16   &  13.70  &   15.15  &   14.29       \\ \cline{2-10}
                     & Sub-pop. size &  151  & 122 &  129  & 6  &   175 &  140  & 155  & 146         \\ \hline    
\multirow{3}{*}{30} & Generations    &  259845.00   &  194660.95   &  206544.43  &   2908.28 &  316523.85   &  215456.53   &  279712.25   &  259487.13          \\ \cline{2-10}
                     & Proportion    &  14.98  	&   11.22  &   11.90  &   0.17   &   18.24  &  12.42  &   16.12   &  14.95    \\ \cline{2-10}
                     & Sub-pop. size &  154 	&  114  & 121 &  4  &   186 &  127  & 165  & 153      \\ \hline    
\multirow{3}{*}{50} & Generations    &  258336.70  &   181412.48   &  183367.28   &  3799.03  & 318174.65   &  188494.30  &   281520.05  &   260924.05         \\ \cline{2-10}
                     & Proportion    &  15.41   &  10.82  &   10.94  &   0.23   &   18.98  &   11.25  &   16.80   &  15.57  \\ \cline{2-10}
                     & Sub-pop. size &  157 &  110 &  112 &  6   &  194 &  115  & 171  & 159        \\ \hline  
\end{tabular}
}
\label{table:generations8X}
}
\end{table*}


\subsubsection{Heterogeneous Size configuration}

Our aim consists in validating the following hypothesis: adapting the sub-population size to the computational power of the heterogeneous cluster nodes presents an improvement in execution time. 


In this work, for a possible offline way to calculate the computational performance of each node, we have used the average number of generations obtained in the HoSi/HeHa configuration for both problems to determine the computational power of the heterogeneous machines. This comparison takes into account all the evolutionary process in a fair manner (proportional to the memory, processor and network usage), instead a traditional benchmark that usually relies only on the CPU speed. Although this is not obviously the best way, it is a possible way to establish the computational power for the experiments of this work and to determine if changing the sub-population size according the computational power reduces the computing time of the whole approach. It should be considered that the contribution of this work is not the way we have computed these sizes, but compare the algorithm with parameters adapted to their power.

Thus, we have used the obtained average number of generations in the previous sub-section (Tables \ref{table:generations4X} and \ref{table:generations8X}) to set proportionally the sizes in the HeSi/HeHa and HeSi/HoHa configurations, by dividing the total number of individuals (1024). Note that, even having two nodes with the same processors and memory (HeN1 and HeN2), they could have different computational power: this may be produced by different operating systems, virtual machine versions, or number of processes being executed (inside a node).



\subsubsection{Adaptive Size configuration}



Finally, in order to validate the hypothesis that adapt the sub-population sizes to computational resources of a heterogeneous cluster leads to decrease of time for obtain the solution, we propose a third configuration. In this experiment, the adaptation of the sub-population size to the computational power of the islands (nodes) is performed during runtime (online).  Each time a node ($N$) receives an individual, it compares its current number of generations ($Gen_{N}$) with the ones of the node who sent the individual (node $N-1$ in the ring). Then, the sub-population size is adapted proportionally to the difference in the number of generations, following the next equation:

\begin{equation}
size'_{N}=\dfrac{Gen_{N}}{Gen_{N-1}}size_{N}
\end{equation}

If the new size is larger than the actual size, new individuals are added to the sub-population cloning random existent ones. Otherwise, the sub-population must be reduced and thus, the worst are removed. The algorithm from Figure \ref{fig:EA} is now updated as shown in Figure \ref{fig:EAadaptive}.

\begin{figure}[htb]

\begin{algorithmic}
\STATE N $\gets$ initialPopulationSize
\STATE population $\gets$ initializePopulation(N)
\WHILE {stop criterion not met}
    \STATE G $\gets$ G + 1
    \STATE parents $\gets$ selection(population)
    \STATE offspring $\gets$ recombination(parents)
    \STATE offspring $\gets$ mutation(offspring)
    \STATE population $\gets$ population + offspring
    \STATE O $\gets$ offspring.size
    \IF {time to migrate}
      \STATE migrants $\gets$ selectMigrants(population)
      \STATE remoteBuffer.send(migrants)
    \ENDIF

    \STATE I $\gets$ 0
    \IF {localBuffer.size $\neq$ zero}
      \STATE immigrants $\gets$ localBuffer.read()
      \STATE iG $\gets$ immigrants.getGeneration()
      \STATE I $\gets$ immigrants.size()
      \STATE population $\gets$ population + immigrants
    \ENDIF

    \STATE population $\gets$ population - population.getTheWorst(O + I)

    \STATE newSize $\gets$ calculateNewSize(iG, G, N)
    \IF {newSize $>$ N}
        \STATE population $\gets$ population + population.getRandomClones(newSize - N)
    \ELSE
        \STATE population $\gets$ population - population.getTheWorst(N - newSize)
    \ENDIF
    \STATE N $\gets$ newSize
\ENDWHILE

\end{algorithmic}
\caption{Pseudo-code of the used dEA: a distributed Genetic Algorithm (dGA) with automatic size adaptation mechanism.}
\label{fig:EAadaptive}
\end{figure}

With this possible online adaptation scheme, each node only requires to receive information of one of the neighbours and not from the whole system. Thus, each node tends to have a number of individuals proportional to their computational power with respect to the other nodes. Experiments on homogeneous cluster do not alter the sub-population sizes, as the number of current generations are equal in all nodes during runtime.

Table \ref{table:parameters} summarizes all the parameters used in the experiments.

\begin{table}
\centering
\caption{Parameters used in all configurations.}
\begin{tabular}{|c|c|} \hline
Name & Value\\ \hline

Crossover type & Two-points crossover \\ \hline
Crossover rate & 0.5\\ \hline
Mutation probability of each gene & 1/individual size\\ \hline
Selection & 2-tournament \\ \hline
Replacement & Worst are removed\\ \hline
Generations to migrate & 64 \\ \hline
Number of individuals to migrate & 1 \\ \hline
Stop criterion & Optimum found \\ \hline
Individual size for MMDP & 150, 240 and 300  \\ \hline
Individual size for OneMax & 5000, 10000 and 15000 \\ \hline
Individual size for Rosenbrock & 10, 30 and 50 \\ \hline
Runs per configuration & 40 \\ \hline
\hline
Total individuals in HoSi and HeSi & 1024\\ \hline \hline
Sub-population size in each node in HoSi (4x) & 256  \\ \hline
Sub-population size in each node in HoSi (8x) & 128  \\ \hline
Sub-population sizes in HeSi & See Tables \ref{table:generations4X} and \ref{table:generations8X}\\ \hline
\hline
Maximum island size in AdSi & 1024 \\ \hline
Minimum island size in AdSi (4x) & 16 \\ \hline
Initial island size in AdSi (4x) & 256 \\ \hline 
Minimum island size in AdSi (8x) & 8 \\ \hline
Initial island size in AdSi (8x) & 128 \\ \hline 
\end{tabular}
\label{table:parameters}
\end{table}

\subsection{Framework}
In order to deal with the operating system and architecture heterogeneity (different operating systems, processors, compilers, etc.), the OSGiLiath framework \cite{SOASOCO}, based in Java, has been used in this work. This is a service-oriented evolutionary framework that automatically configures the services to be used in a local network. In this case, each node offers a migration buffer to accept foreign individuals. Also, in order to reduce bottlenecks in distributed executions, asynchronous communication has been provided to avoid idle time using reception buffers (that is, the algorithm does not wait until new individuals arrive, but the buffers cannot be used again until the reception is done). This kind of communication offers an excellent performance when working with different nodes and operating systems, as demonstrated in \cite{HETEROGENEOUSHARD,AsynchronousMerelo08}. The transmission mechanism is based on ECF Generic server (over TCP)\footnote{\url{http://www.eclipse.org/ecf/}}.  The source code of the algorithms used in this work is available in \url{http://www.osgiliath.org} under a LGPL V3 License. 

%%%%%%%%%%%%%%%%%%  Results  %%%%%%%%%%%%%%%%%%%

\section{Results}
\label{sec:results}



The three main objectives of parallel programming are to tackle large computational problems, increase the performance of algorithms in a finite time, or reduce computational time to solve the problem (reaching the optimum). In this work, we focus in the last objective.
As claimed by Alba and Luque in \cite{EVALUATIONPARALLEL}, assessing the performance of a parallel EA by the number of fitness function evaluations required to attain a solution may be misleading. In our case, for example, the evaluation time is different in each node of the heterogeneous cluster, so the real algorithm speed (in time)could not be reflected correctly. However, the number of evaluations has been included in this section to better understand the results. It is difficult to compare the performance of HoHa and HeHa for the same reason: the evaluation time is different in each system (and even in each node). Thus, in this work, our aim is not making the heterogeneous cluster comparable or better in time than the homogeneous one (because they are, obviously, different), but showing that the same parameter configuration can improve performance in time on heterogeneous clusters and could not have an effect on homogeneous ones.

\begin{table*}
\centering
\caption{Time to reach the optimum (average $\pm$ std. dev. of 30 runs)  for each configuration. and comparison with the base configuration (HoSi): \ding{115} significantly higher. \ding{116} significantly lower. \ding{117} = no significant difference}
\resizebox{14cm}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|} \hline

            \multicolumn{7}{|c|}{MMDP}                                                                \\ \hline
\multicolumn{2}{|c|}{}        &  \multicolumn{3}{|c|}{Heterogeneous Hardware}                                     &  \multicolumn{2}{|c|}{Homogeneous Hardware}                        \\ \hline
\multicolumn{2}{|c|}{}        &  Homogeneous Size        &  Heterogeneous Size            &  Adaptive Size           &  Homogeneous Size        &  Heterogeneous Size            \\ \hline
4  &  150   &  23691.400   $\pm$ 20349.375   &  12123.575   $\pm$ 10535.090   \ding{116}  &  30285.875   $\pm$ 33552.447   \ding{117}  &  11560.800   $\pm$ 26072.141   &  5500.600 $\pm$ 12350.129   \ding{117}  \\ \hline
   &  240   &  347048.675  $\pm$ 192450.028  &  199975.150  $\pm$ 146997.836  \ding{116}  &  417145.600  $\pm$ 269732.017  \ding{117}  &  198431.289  $\pm$ 160166.862  &  358710.025  $\pm$ 248159.908  \ding{115}  \\ \hline
   &  300   &  1061136.564 $\pm$ 373629.143  &  882104.872  $\pm$ 465160.080  \ding{116}  &  1228869.800 $\pm$ 413829.829  \ding{115}  &  1069118.750 $\pm$ 666688.697  &  1264568.050 $\pm$ 647531.978  \ding{117}  \\ \hline
8  &  150   &  11348.200   $\pm$ 6881.718 &  8055.550 $\pm$ 5941.888 \ding{116}  &  11888.525   $\pm$ 12928.340   \ding{117}  &  4314.100 $\pm$ 10816.704   &  6298.025 $\pm$ 12192.268   \ding{117}  \\ \hline
   &  240   &  115213.275  $\pm$ 96734.707   &  106284.375  $\pm$ 66143.938   \ding{117}  &  146685.800  $\pm$ 77926.241   \ding{117}  &  109498.425  $\pm$ 98623.989   &  143193.550  $\pm$ 165182.806  \ding{117}  \\ \hline
   &  300   &  351729.250  $\pm$ 167077.357  &  383083.975  $\pm$ 249783.901  \ding{117}  &  434478.025  $\pm$ 200879.857  \ding{115}  &  521163.050  $\pm$ 304559.018  &  595276.077  $\pm$ 355235.861  \ding{117}  \\ \hline
            \multicolumn{7}{|c|}{OneMax}                                                                 \\ \hline
\multicolumn{2}{|c|}{}        &  \multicolumn{3}{|c|}{Heterogeneous Hardware}                                     &  \multicolumn{2}{|c|}{Homogeneous Hardware}                        \\ \hline
\multicolumn{2}{|c|}{}        &  Homogeneous Size        &  Heterogeneous Size            &  Adaptive Size           &  Homogeneous Size        &  Heterogeneous Size            \\ \hline
4  &  5000  &  105413.125  $\pm$ 2381.213 &  122794.675  $\pm$ 3285.982 \ding{115}  &  137947.400  $\pm$ 2716.859 \ding{116}  &  141176.100  $\pm$ 2493.730 &  109562.650  $\pm$ 2470.219 \ding{116}  \\ \hline
   &  10000 &  427977.875  $\pm$ 10484.174   &  538730.250  $\pm$ 5669.626 \ding{115}  &  329484.825  $\pm$ 9937.048 \ding{116}  &  598873.429  $\pm$ 18937.129   &  437625.225  $\pm$ 7912.703 \ding{116}  \\ \hline
   &  15000 &  1087096.550 $\pm$ 18047.257   &  1415919.250 $\pm$ 20477.570   \ding{115}  &  666451.500  $\pm$ 17877.417   \ding{116}  &  1510952.275 $\pm$ 21107.828   &  1068212.300 $\pm$ 25097.388   \ding{116}  \\ \hline
8  &  5000  &  48739.750   $\pm$ 1171.984 &  54762.400   $\pm$ 779.721  \ding{115}  &  42908.250   $\pm$ 1953.038 \ding{116}  &  85041.450   $\pm$ 1147.126 &  65303.900   $\pm$ 1156.825 \ding{116}  \\ \hline
   &  10000 &  173929.125  $\pm$ 2726.059 &  198988.750  $\pm$ 2777.299 \ding{115}  &  175120.825  $\pm$ 9428.557 \ding{117}  &  320557.300  $\pm$ 4661.152 &  235143.225  $\pm$ 5530.272 \ding{116}  \\ \hline
   &  15000 &  397398.125  $\pm$ 7076.647 &  444353.350  $\pm$ 5952.183 \ding{115}  &  430550.775  $\pm$ 14095.819   \ding{115}  &  738570.900  $\pm$ 10834.293   &  539734.575  $\pm$ 8473.574 \ding{116}  \\ \hline
            \multicolumn{7}{|c|}{Shifted Rosenbrock Function}                                                                 \\ \hline
\multicolumn{2}{|c|}{}        &  \multicolumn{3}{|c|}{Heterogeneous Hardware}                                     &  \multicolumn{2}{|c|}{Homogeneous Hardware}                        \\ \hline
\multicolumn{2}{|c|}{}        &  Homogeneous Size        &  Heterogeneous Size            &  Adaptive Size           &  Homogeneous Size        &  Heterogeneous Size            \\ \hline
4  &  10 &  53352.500   $\pm$ 27765.191   &  63905.825   $\pm$ 26795.632   \ding{115}  &  72567.025   $\pm$ 39079.540   \ding{115}  &  144064.750  $\pm$ 59634.750   &  133317.350  $\pm$ 53464.402   \ding{117}  \\ \hline
   &  30 &  233582.125  $\pm$ 159454.007  &  187654.125  $\pm$ 133253.223  \ding{117}  &  268306.800  $\pm$ 212295.411  \ding{115}  &  225579.325  $\pm$ 76117.466   &  273459.025  $\pm$ 87878.437   \ding{115}  \\ \hline
   &  50 &  431820.850  $\pm$ 294081.294  &  344285.475  $\pm$ 205736.331  \ding{117}  &  461542.775  $\pm$ 300039.633  \ding{117}  &  411643.275  $\pm$ 65338.206   &  553531.700  $\pm$ 198818.335  \ding{115}  \\ \hline
8  &  10 &  136191.400  $\pm$ 79714.749   &  99235.500   $\pm$ 64553.242   \ding{116}  &  64515.275   $\pm$ 29709.340   \ding{116}  &  117962.75   $\pm$ 35741.64  &  135527.725  $\pm$ 38170.154  \ding{117}  \\ \hline
   &  30 &  220603.675  $\pm$ 99104.552   &  139196.425  $\pm$ 137089.954  \ding{116}  &  158356.275  $\pm$ 105886.233  \ding{116}  &  216103.4 $\pm$    21323.04  &  224968.575  $\pm$ 39149.218  \ding{117}  \\ \hline
   &  50 &  252010.950  $\pm$ 106664.020  &  210020.025  $\pm$ 128371.826  \ding{116}  &  283033.825  $\pm$ 169912.769  \ding{117}  &  312136.7 $\pm$    81755.296  &  372139.1 $\pm$ 63905.477  \ding{115}  \\ \hline


\end{tabular}
}
\label{tab:resultsTIMEall}
\end{table*}


\begin{table*}
\centering
\caption{Number of evaluations (average $\pm$ std. dev. of 30 runs)  for each configuration. and comparison with the base configuration (HoSi): \ding{115} significantly higher. \ding{116} significantly lower. \ding{117} = no significant difference}
\resizebox{14cm}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|} \hline

            \multicolumn{7}{|c|}{MMDP}                                                                         \\ \hline
\multicolumn{2}{|c|}{}        &  \multicolumn{3}{|c|}{Heterogeneous Hardware}                                           &  \multicolumn{2}{|c|}{Homogeneous Hardware}                           \\ \hline
\multicolumn{2}{|c|}{}        &  Homogeneous Size        &  Heterogeneous Size               &  Adaptive Size              &  Homogeneous Size        &  Heterogeneous Size               \\ \hline
4  &  150   &  3321990.4   $\pm$  3640752.796  &  1318095.1   $\pm$ 1945414.545  \ding{116}     &  4495540  $\pm$    6076330.842  \ding{117}     &  1314204.8   $\pm$      2963639.113 &       593926.5 $\pm$        1344934.779  \ding{115}     \\ \hline
   &  240   &  43465120    $\pm$ 24350006.278  &  24865324.65 $\pm$ 18722527.744  \ding{116}     &  51918846 $\pm$    33924352.604   \ding{117}     &  20141093.052  $\pm$ 16264249.553  &  29297987.6  $\pm$  20289861.289  \ding{115}     \\ \hline
   &  300   &  112163404.8 $\pm$ 40760641.082  &  94824634.45 $\pm$ 43726167.176   \ding{117}     &  126292378.1 $\pm$ 42772727.879  \ding{117}     &  91090851.2  $\pm$       56752255.807  &  86123573.45 $\pm$  44098884.083  \ding{117}     \\ \hline
8  &  150   &  2212630.4   $\pm$  2402352.533  &  1143879.95  $\pm$ 2251876.900  \ding{116}     &  2133441  $\pm$    4344042.555  \ding{117}     &  769020.8 $\pm$          1968495.881  &  1045242.85  $\pm$  2044957.133  \ding{117}     \\ \hline
   &  240   &  33820148.8  $\pm$ 29651523.050  &  33028780.4  $\pm$ 21576055.525  \ding{117}     &  44303976.55 $\pm$ 24510225.672  \ding{117}     &  17062513.6  $\pm$       15386147.559  &  19772115.75 $\pm$  22806815.899  \ding{115}     \\ \hline
   &  300   &  85830209.6  $\pm$ 41366325.301  &  92198558.85 $\pm$ 60854343.022  \ding{117}     &  110130432.3 $\pm$ 50703413.222  \ding{117}     &  71996897.6  $\pm$       42054877.579  &  72278155.12  $\pm$ 43134165.117  \ding{117}     \\ \hline
            \multicolumn{7}{|c|}{OneMax}                                                                          \\ \hline
\multicolumn{2}{|c|}{}        &  \multicolumn{3}{|c|}{Heterogeneous Hardware}                                           &  \multicolumn{2}{|c|}{Homogeneous Hardware}                           \\ \hline
\multicolumn{2}{|c|}{}        &  Homogeneous Size        &  Heterogeneous Size               &  Adaptive Size              &  Homogeneous Size        &  Heterogeneous Size               \\ \hline
4  &  5000  &  762841.6    $\pm$ 16566.019  &  910877.55   $\pm$ 21772.716  \ding{115}     &  916197.45   $\pm$  23011.217  \ding{115}     &  911238.4 $\pm$      16014.892  &  681001.65   $\pm$ 14423.215  \ding{116}     \\ \hline
   &  10000 &  1509209.6   $\pm$ 38518.094  &  1845115.85  $\pm$ 22200.410  \ding{115}     &  1057320.85  $\pm$  35026.167  \ding{116}     &  1194003.4   $\pm$   29300.446  &  874954.2875 $\pm$ 15848.262  \ding{115}     \\ \hline
   &  15000 &  2271324.8   $\pm$ 46631.097  &  2854326.95  $\pm$ 43874.970  \ding{115}     &  1388917.95  $\pm$  46410.417  \ding{116}     &  3021651.76   $\pm$ 42266.984  &  2134754.8   $\pm$ 49238.440  \ding{116}     \\ \hline
8  &  5000  &  1022355.2   $\pm$ 27173.857  &  1191468.65  $\pm$ 18848.880  \ding{115}     &  876971.15   $\pm$  53281.452  \ding{116}     &  1060456     $\pm$  14202.837  &  783309.6 $\pm$ 13667.556   \ding{115}     \\ \hline
   &  10000 &  1990523.2   $\pm$ 33071.283  &  2310726.55  $\pm$ 35550.662  \ding{115}     &  1943270.05  $\pm$ 109736.643  \ding{117}     &  2104908.8   $\pm$ 31371.894   &  1500722.6   $\pm$ 32442.965  \ding{115}     \\ \hline
   &  15000 &  2992969.6   $\pm$ 56187.847  &  3447571.1   $\pm$ 49039.431  \ding{115}     &  3164844.6   $\pm$ 112128.458  \ding{115}     &  3189692.8   $\pm$ 48537.530  &  2275397.35  $\pm$ 37727.264  \ding{115}     \\ \hline
            \multicolumn{7}{|c|}{Shifted Rosenbrock Function}                                                                          \\ \hline
\multicolumn{2}{|c|}{}        &  \multicolumn{3}{|c|}{Heterogeneous Hardware}                                           &  \multicolumn{2}{|c|}{Homogeneous Hardware}                           \\ \hline
\multicolumn{2}{|c|}{}        &  Homogeneous Size        &  Heterogeneous Size               &  Adaptive Size              &  Homogeneous Size        &  Heterogeneous Size               \\ \hline
4  &  10 &  22355824    $\pm$ 12904481.344  &  30266762.2  $\pm$ 13813479.010  \ding{115}     &  33547324.1  $\pm$ 19863897.742  \ding{115}     &  37341065.6  $\pm$ 16047434.437  &  30153466.3  $\pm$ 12576709.796  \ding{116}     \\ \hline
   &  30 &  69538364.8  $\pm$ 48304416.398  &  59728382.4  $\pm$ 43501832.250  \ding{117}     &  88388447.15 $\pm$ 71317459.548  \ding{117}     &  46161708.8  $\pm$ 15934477.472  &  47148156.65 $\pm$ 15375823.195   \ding{115}     \\ \hline
   &  50 &  99155420.8  $\pm$ 68311226.248  &  84094769.2  $\pm$ 50765873.592  \ding{117}     &  117038182.7 $\pm$ 77175066.543   \ding{117}     &  68457408    $\pm$ 11020886.010  &  76027870.2  $\pm$ 27583556.014  \ding{117}     \\ \hline
8  &  10 &  93296334.4  $\pm$ 56652940.874  &  72985728.55 $\pm$ 50098410.959  \ding{117}     &  27464762.4  $\pm$ 14325583.024  \ding{116}     &  29918763.2  $\pm$  9461803.701  &  33237782.15 $\pm$  9691812.091  \ding{117}     \\ \hline
   &  30 &  124082083.2 $\pm$ 56744693.550  &  72921017.2  $\pm$ 44154090.205  \ding{116}     &  76865581.95 $\pm$ 51105012.648  \ding{116}     &  50067939.2  $\pm$  5035938.893  &  49326413.85 $\pm$  8797658.463  \ding{117}     \\ \hline
   &  50 &  118758705.6 $\pm$ 51402559.633  &  105989246.15$\pm$ 66396359.623  \ding{117}     &  119654538.5 $\pm$ 73281480.758  \ding{117}     &  64545646.4  $\pm$ 17131983.221  &  68750401.21 $\pm$ 17422479.166  \ding{117}     \\ \hline


\end{tabular}
}
\label{tab:resultsEVALSall}
\end{table*}




\begin{figure}[ht]
\centering

\subfigure{
   \includegraphics[scale =0.3] {boxplots/MMDP_150_TIME.eps}
   \label{fig:150time}
 }
\subfigure{
   \includegraphics[scale =0.3] {boxplots/MMDP_240_TIME.eps}
   \label{fig:240time}
 }
 \subfigure{
   \includegraphics[scale =0.3] {boxplots/MMDP_300_TIME.eps}
   \label{fig:240time}
 }
\caption{Time to obtain the optimum in the MMDP problem (milliseconds).}
\label{fig:timeMMDP}
\end{figure}

\begin{figure}[ht]
\centering

\subfigure{
   \includegraphics[scale =0.3] {boxplots/ONEMAX_5000_TIME.eps}
   \label{fig:5000time}
 }
\subfigure{
   \includegraphics[scale =0.3] {boxplots/ONEMAX_10000_TIME.eps}
   \label{fig:10000time}
 }
 \subfigure{
   \includegraphics[scale =0.3] {boxplots/ONEMAX_15000_TIME.eps}
   \label{fig:15000time}
 }
\caption{Time to obtain the optimum in the OneMax problem (milliseconds).}
\label{fig:timeONEMAX}
\end{figure}

\begin{figure}[ht]
\centering

\subfigure{
   \includegraphics[scale =0.3] {boxplots/ROSENBROCK_10_TIME.eps}
   \label{fig:10time}
 }
\subfigure{
   \includegraphics[scale =0.3] {boxplots/ROSENBROCK_30_TIME.eps}
   \label{fig:30time}
 }
 \subfigure{
   \includegraphics[scale =0.3] {boxplots/ROSENBROCK_50_TIME.eps}
   \label{fig:50time}
 }
\caption{Time needed to obtain the optimum in the Rosenbrock function problem (milliseconds).}
\label{fig:timeROSENBROCK}
\end{figure}










\begin{figure}[ht]
\centering

\subfigure{
   \includegraphics[scale =0.3] {boxplots/MMDP_150_EVALS.eps}
   \label{fig:150evals}
 }
\subfigure{
   \includegraphics[scale =0.3] {boxplots/MMDP_240_EVALS.eps}
   \label{fig:240evals}
 }
 \subfigure{
   \includegraphics[scale =0.3] {boxplots/MMDP_300_EVALS.eps}
   \label{fig:240evals}
 }
\caption{Evaluations to obtain the optimum in the MMDP problem.}
\label{fig:evalsMMDP}
\end{figure}

\begin{figure}[ht]
\centering

\subfigure{
   \includegraphics[scale =0.3] {boxplots/ONEMAX_5000_EVALS.eps}
   \label{fig:5000evals}
 }
\subfigure{
   \includegraphics[scale =0.3] {boxplots/ONEMAX_10000_EVALS.eps}
   \label{fig:10000evals}
 }
 \subfigure{
   \includegraphics[scale =0.3] {boxplots/ONEMAX_15000_EVALS.eps}
   \label{fig:15000evals}
 }
\caption{Evaluations to obtain the optimum in the OneMax problem.}
\label{fig:evalsONEMAX}
\end{figure}

\begin{figure}[ht]
\centering

\subfigure{
   \includegraphics[scale =0.3] {boxplots/ROSENBROCK_10_EVALS.eps}
   \label{fig:10evals}
 }
\subfigure{
   \includegraphics[scale =0.3] {boxplots/ROSENBROCK_30_EVALS.eps}
   \label{fig:30evals}
 }
 \subfigure{
   \includegraphics[scale =0.3] {boxplots/ROSENBROCK_50_EVALS.eps}
   \label{fig:50evals}
 }
\caption{Evaluations needed to obtain the optimum in the Rosenbrock function problem.}
\label{fig:evalsROSENBROCK}
\end{figure}




% No tiene mucho sentido hablar de los resultados de los dos problemas
% por separado, porque el objetivo no es resolver ese problema, sino
% probar cómo funciona en algoritmo. Se pueden comentar los dos juntos
% y se debería hacer - JJ FERGU: okis, poniéndolo todo junto.

Table \ref{tab:resultsTIMEall} shows the execution times for the all the problems and configurations, while Table \ref{tab:resultsEVALSall} shows the total number of evaluations. To compare between two methods (HoSi and HeSi in the homogeneous cluster) a Wilcoxon test has been applied. For a three methods comparison (HoSi, HeSi and AdSi on heterogeneous cluster) a Kruskal-Wallis test has been used. A mark in the tables indicates if the comparison with the base configuration of each cluster (HoSi/HeHa and HoSi/HoHa respectively) is significantly higher (\ding{115}), significantly lower  (\ding{116}), or no significant difference found (\ding{117}).

These results, grouped by problem, are also plotted in the boxplots of Figures \ref{fig:timeMMDP}, \ref{fig:timeONEMAX} and \ref{fig:timeROSENBROCK} (time) and Figures \ref{fig:evalsMMDP}, \ref{fig:evalsONEMAX} and \ref{fig:evalsROSENBROCK} (evaluations).

The results show that there exist significant difference depending on the problem, configuration or number of nodes. For example, for problem MMDP in the HeHa system, adapting offline the sub-population to the computational
 power of each node makes the algorithm finish significantly earlier in all instances of the problem in the 4X configuration, and also, needing a lower number of evaluations to reach the solution. However, in the 8X configuration the time is significantly lower in the 150 instance. This can be explained because the nature of this problem, EXPLICAR AQUI PORQUE PASA (ENCUENTRA SOLUCION PRONTO)

%Maribel, esto va a ser duro de explicar, quizás porque al haber más nodos, hay muchos más individuos y por lo tanto más diversidad y la diversidad es clave en este problema porque si no hay bloques buenos, da igual el mucho tiempo que lo evoluciones, que no se llegan a conseguir, pero con muchos individuos, tienes mucha diversidad y tienes disponibles bloques buenos que van combinándose mucho más rápido.

 On the other hand, in the HoHa system,
 setting the same sub-population heterogeneous sizes makes no difference in time, that is, changing this parameter has no influence in the
 algorithm's performance.








% este junto con el anterior y un párrafo al final de los dos a
% comentar conjuntamente los resultados obtenidos. - JJ FERGU: Comentando conjuntamente



Results for this problem are shown in Table \ref{tab:onemaxresults} and Figures  \ref{fig:timeOneMax} and \ref{fig:evalsOneMax}. In this case, adapting offline the sub-population sizes significantly decreases  the running time for solving it in the heterogeneous cluster, but this time, the number of evaluations is increased (see statistical significance in Table \ref{tab:significanceONEMAX}). In the homogeneous system, the effect of changing the sub-population sizes is clearer, and this time the number of evaluations (and therefore, the time) are reduced (both significantly). 

The efficiency on OneMax problem depends mainly on the ability to mix
the building-blocks, and less on the genetic diversity and size of the
population (as with MMDP). No genetic diversity is particularly
required. When properly tuned, a simple Genetic Algorithm is able to
solve OneMax in linear time. Sometimes, problems like OneMax are used
as control functions, in order to check if very efficient algorithms
on hard functions fail on easier ones. 

%As it can be seen in Figure
%\ref{fig:gensonemaxhomosize}, the average fitness of all sub-populations
%are increasing in linear way in the HoSi/HeHa configuration. However,
%the slower node evaluates extremely fewer times.  On the other
%side, in Figure \ref{fig:gensonemaxheterosize}, smaller sub-population
%sizes make that slower nodes increase the number of evaluations,
%but the average fitness is also maintained in linear way (and in
%smaller increase rate) between migrations. Nevertheless, the other
%nodes still perform a higher number of evaluations. That is the
%reason why the number of evaluations is higher in HeHa, and lower in
%HoHa. Computational time is more efficiently spent in faster nodes,
%having a higher chance to cross the individuals. In addition, due to
%the larger size of  individuals in the OneMax problem (5000 bits
%vs. 150 of the MMDP), the transmission time is larger, (white gaps in the
%figures). It also implies that HeN4 sends its best individual to
%HeN1 in an extremely large amount of time when using HoSi (every 64
%generations). 





%\begin{figure}[ht]
%\centering

%\subfigure[Heterogeneous cluster]{
%   \includegraphics[scale =0.35] {9a.eps}
%   \label{fig:subfig1}
% }
%\subfigure[Homogeneous cluster]{
%   \includegraphics[scale =0.35] {9b.eps}
%   \label{fig:subfig2}
% }
%\caption{Time to obtain the optimum in the OneMax problem (milliseconds).}
%\label{fig:timeOneMax}
%\end{figure}

EXPLICAR LOS RESULTADOS DE ONEMAX Y ROSENBROCK

Summarizing, adapting the population sizes to the
  computational power of each machine (offline and online) can reduce
  the time to obtain the optimum. The same heterogeneous fixed sizes
  in the homogeneous cluster does not produces a significant decrease
  of running time, so the improvement is produced by the heterogeneity
  and not for the different island sizes. Also, the AdSi proposal is
  not applicable here because there are no differences of generations
  during runtime.

\subsection{Adaptive size}

ANALISIS DEL TAMAÑO ADAPTATIVO






\subsection{Running time analysis}

This sub-section analyses the time spent by each node of the clusters in every stage of the EA for each configuration with fixed sizes (HoSi and HeSi). Figures \ref{fig:MMDPbars}, \ref{fig:ONEMAXbars} and \ref{fig:ROSENBROCKbars} graphically compare these results. As it can be seen, the migration is the most time consuming operation in all configurations, being the migration in HeHa more expensive than in HoHa. This happens because we are using the multi-purpose laboratory network to communicate the nodes, instead of the specific one used in the HoHa system. Note that the standard deviation of the migration is larger in the HeHa cluster because the network is having real conditions of traffic during the experiment. In the MMDP problem (Table \ref{tab:mmdptimes}) changing the sub-population size does not affect the migration time, but it affects the rest of the algorithm's stages. However, with larger data communications (individuals of 5000 elements of the OneMax problem), the sub-population size affects the migration time of all nodes. This might be due to the synchronization of migration buffers: if the slowest machine is sending/receiving, bottlenecks can be propagated (as it can be seen in Figure \ref{fig:gensonemaxhomosize}). 

FALTAN LAS FIGURAS

Results also show how the stages of the algorithms depends on the node
of execution. For example, recombination needs more time than mutation
in CUAAAAAAALES problems only in the node HeN4. The reason might be the
creation of new objects (memory allocation), which in Java and in
limited memory (and swapping) requires more time than the iteration of
elements previously created (for example, in the mutation). Adapting
the sub-population size makes the slower node of HeHa behave in similar
way than the other nodes (same time in each stage). Moreover, the size
of the individuals affects to some parts of the EA; for example, in 
OneMax the mutation requires more time than the replacement. However,
it must be taken into account that the duration of each part of the
algorithm is not related to the time to attain the optimum, but rather to
how the diversity and search guidance is maintained in the whole system.  



\section{Conclusions and future work}




In this paper we describe a study on the adaptation of the sub-population sizes of a distributed EA to the computational power of the different nodes of an heterogeneous cluster. Two adaptation schemes (offline and online) that use information of the computational load of the algorithm have been tested.
 				% No. Tratamos de
                                % probar un objetivo, dilo aquí!!!!!
Results show that adapting (online or offline) the sub-population size to the computational power of each node in the heterogeneous cluster can yield significantly
better results in time than keeping the same parameter in all nodes. This advantage is due to the combination of the heterogeneous parameters with the heterogeneity of the machines. On the contrary, the same (heterogeneous) parameter setting in all islands of the homogeneous cluster could not improve the results than considering the same parameter value in all nodes.


 % o sea, tener
                                % maquinas heterogéneas y parámetros
                                % heterogéneos es mejor porque usamos
                                % parámetros heterogéneos en máquinas
                                % heterogéneas. Di en qué puede
                                % influir eso en la mejora de los
                                % resultados y discute por qué podría
                                % ser así y propón experimentos para
                                % probar que efectivamente se trata de
                                % eso. - JJ

Furthermore, changing the sub-population size affects to stages
of the algorithm that are independent of this parameter, such as
the migration. The sub-population size adaptation is also affected by the problem to solve.

In this work, as a possible offline parameter setting, we have calculated the computational power of each node proportionally 
to the average number of generations of the homogeneous parameter set. Moreover, a possible way to adapt 
online the sub-population sizes has been performed comparing the current generation with
 the neighbour generation. These results are a promising starting for adapting EAs to the
performance of each execution node, using more adequate benchmarks or in a dynamic way. 
% Falta una discusión sobre si las mejoras se deben exclusivamente al
% nÃºmero de evaluaciones o hay algÃºn otro factor ¿menos overhead?
% ¿nodos más rápidos? - JJ FERGU: no, de hecho el nÃºmero de evaluaciones no siempre disminuye, lo digo arriba.

In future, other parameters such as migration rate or
crossover probability could be adapted to the execution
nodes. Other appropriate benchmarks to analyse the algorithm will be also used to lead to automatic
parameter adaptation in runtime (online), with different nodes entering or
exiting in the topology, or adapting the parameters to the current load of the
system. 

\section*{Acknowledgements}
This work has been supported in part by SIPESCA (Programa Operativo FEDER de Andalucía 2007-2013), TIN2011-28627-C04-02 (Spanish Ministry of Economy and Competitivity), SPIP2014-01437 (Direcci{\'o}n General de Tr{\'a}fico), PRY142/14 (Fundaci{\'o}n P{\'u}blica Andaluza Centro de Estudios Andaluces en la IX Convocatoria de Proyectos de Investigaci{\'o}n) and PYR-2014-17 GENIL project (CEI-BIOTIC Granada).



%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% References
%%
%% Following citation commands can be used in the body text:
%% Usage of \cite is as follows:
%%   \cite{key}         ==>>  [#]
%%   \cite[chap. 2]{key} ==>> [#, chap. 2]
%%

%% References with bibTeX database:

%\bibliographystyle{elsarticle-num}
%\bibliography{AMIVITAL-ESA}

%% Authors are advised to submit their bibtex database files. They are
%% requested to list a bibtex style file in the manuscript if they do
%% not want to use elsarticle-num.bst.

%% References without bibTeX database:

% \begin{thebibliography}{00}

%% \bibitem must have the following form:
%%   \bibitem{key}...
%%

% \bibitem{}

% \end{thebibliography}




\bibliographystyle{plain}
\bibliography{heterogeneous}
%Descomentar abajo cuando comentes arriba
%\section*{References}

%\begin{thebibliography}{10}

%\bibitem{AutomaticallyConfiguringStyles12}
%James Styles, HolgerH. Hoos, and Martin MâÂºller.
%\newblock Automatically configuring algorithms for scaling performance.
%\newblock In Youssef Hamadi and Marc Schoenauer, editors, {\em Learning and
%  Intelligent Optimization}, Lecture Notes in Computer Science, pages 205--219.
%  Springer Berlin Heidelberg, 2012.

%\end{thebibliography}

\end{document}

%%
%% End of file `elsarticle-template-num.tex'.
